{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5dbd8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb71fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableNormalDistribution(nn.Module):\n",
    "    LOG_SQRT2PI = np.log(np.sqrt(2 * np.pi))\n",
    "\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "        self.register_buffer('eps', torch.Tensor(self.mu.shape))\n",
    "        self.sigma = None\n",
    "        self.w = None\n",
    "\n",
    "    def sample(self):\n",
    "        self.eps.data.normal_()\n",
    "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
    "        self.w = self.mu + self.sigma * self.eps\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self):\n",
    "        assert (self.w is not None), \"You can only have a log posterior for W if you've already sampled it\"\n",
    "\n",
    "        log_posteriors = -TrainableNormalDistribution.LOG_SQRT2PI - torch.log(self.sigma) - (((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)) - 0.5\n",
    "        return log_posteriors.sum()\n",
    "\n",
    "\n",
    "class PriorWeightDistribution(nn.Module):\n",
    "    # Calculates a Scale Mixture Prior distribution for the prior part of the complexity cost on Bayes by Backprop paper\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.dist1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.dist2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w):\n",
    "        prob_n1 = torch.exp(self.dist1.log_prob(w))\n",
    "        prob_n2 = torch.exp(self.dist2.log_prob(w))\n",
    "\n",
    "        # Prior of the mixture distribution, adding 1e-6 prevents numeric problems with log(p) for small p\n",
    "        prior_pdf = (self.pi * prob_n1 + (1 - self.pi) * prob_n2) + 1e-6\n",
    "\n",
    "        return (torch.log(prior_pdf) - 0.5).sum()\n",
    "\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0, prior_dist=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # our main parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "        # parameters for the scale mixture prior\n",
    "        self.prior_sigma_1 = prior_sigma_1\n",
    "        self.prior_sigma_2 = prior_sigma_2\n",
    "        self.prior_pi = prior_pi\n",
    "        self.prior_dist = prior_dist\n",
    "\n",
    "        # Variational weight parameters and sample\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_mu_init, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_rho_init, 0.1))\n",
    "        self.weight_sampler = TrainableNormalDistribution(self.weight_mu, self.weight_rho)\n",
    "\n",
    "        # Variational bias parameters and sample\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(posterior_mu_init, 0.1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(posterior_rho_init, 0.1))\n",
    "        self.bias_sampler = TrainableNormalDistribution(self.bias_mu, self.bias_rho)\n",
    "\n",
    "        # Priors (as BBP paper)\n",
    "        self.weight_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n",
    "        self.bias_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample the weights and forward it\n",
    "        w = self.weight_sampler.sample()\n",
    "\n",
    "        if self.bias:\n",
    "            b = self.bias_sampler.sample()\n",
    "            b_log_posterior = self.bias_sampler.log_posterior()\n",
    "            b_log_prior = self.bias_prior_dist.log_prior(b)\n",
    "        else:\n",
    "            b = torch.zeros((self.out_features), device=x.device)\n",
    "            b_log_posterior = 0\n",
    "            b_log_prior = 0\n",
    "\n",
    "        # Get the complexity cost\n",
    "        self.log_variational_posterior = self.weight_sampler.log_posterior() + b_log_posterior\n",
    "        self.log_prior = self.weight_prior_dist.log_prior(w) + b_log_prior\n",
    "\n",
    "        # print(x.shape, w.shape, b.shape)\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        return self.log_variational_posterior - self.log_prior\n",
    "\n",
    "\n",
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75be44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesianLinear(\n",
    "                in_features, in_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(\n",
    "                in_features, in_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(\n",
    "                in_features, out_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            kl += getattr(module, 'kl_divergence', 0) if module != self else 0\n",
    "        return kl\n",
    "\n",
    "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
    "        loss = 0\n",
    "        for _ in range(num_samples):\n",
    "            outputs = self(inputs)\n",
    "            contr1 = criterion(outputs, labels)\n",
    "            contr2 = self.kl_divergence * complexity_cost_weight\n",
    "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
    "            loss += contr1\n",
    "        return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bb4cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f33be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
    "\n",
    "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(test_loader))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "            # print(f\"Predictions: {preds}, Targets: {target}\")\n",
    "\n",
    "    total = len(test_loader.dataset)\n",
    "    error = (total - correct) / total\n",
    "\n",
    "    print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
    "    return total_loss / total, error\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf8a0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd095371",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
    "\n",
    "kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 1,\n",
    "    'pin_memory': True,\n",
    "    'pin_memory_device': 'cuda',\n",
    "    'generator': generator,\n",
    "}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61306f93",
   "metadata": {},
   "source": [
    "# Grid search with wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wandb-api-key')\n",
    "\n",
    "wandb.login(key=key)\n",
    "\n",
    "\n",
    "def train_wrapper():\n",
    "    with wandb.init(project=\"asi-paper\") as run:\n",
    "        model = train_mnist(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=50,\n",
    "            lr=run.config.lr,\n",
    "            num_samples=run.config.sample_nbr,\n",
    "            pi=run.config.pi,\n",
    "            minus_log_sigma1=run.config.min_log_sigma1,\n",
    "            minus_log_sigma2=run.config.min_log_sigma2,\n",
    "            use_wandb=True\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "    'name': \"sweep-mnist\",\n",
    "    \"parameters\": {\n",
    "        \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
    "        \"sample_nbr\": {'values': [1, 2, 5, 10]},\n",
    "        \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "        \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
    "        \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "wandb.agent(sweep_id, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b83bc6",
   "metadata": {},
   "source": [
    "# Manual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 8903/10000 (89.03%)\n",
      "Epoch 1/10, Train Loss: 217.0029, Val Loss: 1588.3174, Val Error: 0.1097\n",
      "Correct: 9290/10000 (92.90%)\n",
      "Epoch 2/10, Train Loss: 195.7099, Val Loss: 1656.0879, Val Error: 0.0710\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mtrain_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_log_sigma1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_log_sigma2\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"mnist_model.pt\")\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_mnist\u001b[39m\u001b[34m(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb)\u001b[39m\n\u001b[32m      9\u001b[39m criterion = nn.CrossEntropyLoss(reduction=\u001b[33m'\u001b[39m\u001b[33msum\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# if use_wandb:\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#     wandb.init(project=\"asi-paper\", name=\"mnist\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m         val_loss, val_error = evaluate(model, val_loader, criterion)\n\u001b[32m     55\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, num_samples)\u001b[39m\n\u001b[32m     11\u001b[39m kl_weight = minibatch_weight(batch_idx, \u001b[38;5;28mlen\u001b[39m(train_loader))\n\u001b[32m     13\u001b[39m loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m optimizer.step()\n\u001b[32m     18\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# model = train_mnist(train_loader, val_loader, epochs=10, lr=0.01, num_samples=5, pi=0.3, minus_log_sigma1=2, minus_log_sigma2=6)\n",
    "# torch.save(model.state_dict(), \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:01.237921Z",
               "iopub.status.busy": "2025-05-16T08:45:01.237304Z",
               "iopub.status.idle": "2025-05-16T08:45:14.741127Z",
               "shell.execute_reply": "2025-05-16T08:45:14.740575Z"
            },
            "papermill": {
               "duration": 13.509038,
               "end_time": "2025-05-16T08:45:14.742515",
               "exception": false,
               "start_time": "2025-05-16T08:45:01.233477",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "from typing import Optional\n",
            "import torch\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "from torchvision import datasets\n",
            "from torchvision.transforms import v2 as transforms\n",
            "import numpy as np\n",
            "import wandb\n",
            "import time\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd\n",
            "from sklearn.preprocessing import StandardScaler"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "generator = torch.Generator().manual_seed(42)\n",
            "np.random.seed(42)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.759611Z",
               "iopub.status.busy": "2025-05-16T08:45:14.759385Z",
               "iopub.status.idle": "2025-05-16T08:45:14.770934Z",
               "shell.execute_reply": "2025-05-16T08:45:14.770461Z"
            },
            "papermill": {
               "duration": 0.015994,
               "end_time": "2025-05-16T08:45:14.771946",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.755952",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "\n",
            "class GaussianVariational(nn.Module):\n",
            "\n",
            "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor) -> None:\n",
            "        super().__init__()\n",
            "\n",
            "        self.mu = nn.Parameter(mu)\n",
            "        self.rho = nn.Parameter(rho)\n",
            "\n",
            "        self.w = None\n",
            "        self.sigma = None\n",
            "\n",
            "        self.normal = torch.distributions.Normal(0, 1)\n",
            "\n",
            "    def sample(self) -> torch.Tensor:\n",
            "        device = self.mu.device\n",
            "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
            "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
            "        self.w = self.mu + self.sigma * epsilon\n",
            "\n",
            "        return self.w\n",
            "\n",
            "    def log_posterior(self) -> torch.Tensor:\n",
            "        assert self.w is not None\n",
            "\n",
            "        log_const = np.log(np.sqrt(2 * np.pi))\n",
            "        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
            "        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n",
            "\n",
            "        return log_posterior.sum()\n",
            "\n",
            "\n",
            "class ScaleMixture(nn.Module):\n",
            "\n",
            "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
            "        super().__init__()\n",
            "\n",
            "        self.pi = pi\n",
            "        self.sigma1 = sigma1\n",
            "        self.sigma2 = sigma2\n",
            "\n",
            "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
            "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
            "\n",
            "    def log_prior(self, w: torch.Tensor) -> torch.Tensor:\n",
            "        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n",
            "        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n",
            "\n",
            "        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n",
            "        log_prob = torch.log(p_scalemixture).sum()\n",
            "\n",
            "        return log_prob\n",
            "\n",
            "\n",
            "class BayesianModule(nn.Module):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BayesLinear(BayesianModule):\n",
            "\n",
            "    def __init__(self,\n",
            "                 in_features: int,\n",
            "                 out_features: int,\n",
            "                 prior_pi: Optional[float] = 0.5,\n",
            "                 prior_sigma1: Optional[float] = 1.0,\n",
            "                 prior_sigma2: Optional[float] = 0.0025) -> None:\n",
            "        super().__init__()\n",
            "\n",
            "        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2, generator=generator)\n",
            "        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0, generator=generator)\n",
            "\n",
            "        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2, generator=generator)\n",
            "        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0, generator=generator)\n",
            "\n",
            "        self.w_posterior = GaussianVariational(w_mu, w_rho)\n",
            "        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n",
            "\n",
            "        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
            "        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
            "\n",
            "        self.kl_divergence = 0.0\n",
            "\n",
            "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
            "        w = self.w_posterior.sample()\n",
            "        b = self.bias_posterior.sample()\n",
            "\n",
            "        w_log_prior = self.w_prior.log_prior(w)\n",
            "        b_log_prior = self.bias_prior.log_prior(b)\n",
            "\n",
            "        w_log_posterior = self.w_posterior.log_posterior()\n",
            "        b_log_posterior = self.bias_posterior.log_posterior()\n",
            "\n",
            "        total_log_prior = w_log_prior + b_log_prior\n",
            "        total_log_posterior = w_log_posterior + b_log_posterior\n",
            "        self.kl_divergence = total_log_posterior - total_log_prior\n",
            "\n",
            "        return F.linear(x, w, b)\n",
            "\n",
            "\n",
            "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
            "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)\n",
            "\n",
            "\n",
            "def variational_estimator(nn_class):\n",
            "\n",
            "    @property\n",
            "    def kl_divergence(self):\n",
            "        kl = 0\n",
            "        for module in self.modules():\n",
            "            if isinstance(module, BayesianModule):\n",
            "                kl += module.kl_divergence\n",
            "\n",
            "        return kl\n",
            "\n",
            "    setattr(nn_class, \"kl_divergence\", kl_divergence)\n",
            "\n",
            "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
            "        loss = 0\n",
            "        for _ in range(num_samples):\n",
            "            outputs = self(inputs)\n",
            "            contr1 = criterion(outputs, labels)\n",
            "            contr2 = self.kl_divergence * complexity_cost_weight\n",
            "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
            "            loss += contr1 + contr2\n",
            "        return loss / num_samples\n",
            "\n",
            "    setattr(nn_class, \"sample_elbo\", sample_elbo)\n",
            "\n",
            "    return nn_class"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.777658Z",
               "iopub.status.busy": "2025-05-16T08:45:14.777453Z",
               "iopub.status.idle": "2025-05-16T08:45:14.834303Z",
               "shell.execute_reply": "2025-05-16T08:45:14.833574Z"
            },
            "papermill": {
               "duration": 0.060988,
               "end_time": "2025-05-16T08:45:14.835492",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.774504",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(f\"Using device: {device}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "151d030c",
         "metadata": {
            "papermill": {
               "duration": 0.002498,
               "end_time": "2025-05-16T08:45:14.840678",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.838180",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "# MNIST classification"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a2588383",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.846307Z",
               "iopub.status.busy": "2025-05-16T08:45:14.846093Z",
               "iopub.status.idle": "2025-05-16T08:45:14.852070Z",
               "shell.execute_reply": "2025-05-16T08:45:14.851508Z"
            },
            "papermill": {
               "duration": 0.010054,
               "end_time": "2025-05-16T08:45:14.853120",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.843066",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "@variational_estimator\n",
            "class MNISTModel(nn.Module):\n",
            "\n",
            "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
            "        super().__init__()\n",
            "\n",
            "        self.layers = nn.Sequential(\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                1200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                1200,\n",
            "                1200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                1200,\n",
            "                out_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2,\n",
            "            ),\n",
            "            nn.Softmax(dim=1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.layers(x)\n",
            "        # print(x)\n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.859020Z",
               "iopub.status.busy": "2025-05-16T08:45:14.858809Z",
               "iopub.status.idle": "2025-05-16T08:45:14.866668Z",
               "shell.execute_reply": "2025-05-16T08:45:14.866118Z"
            },
            "papermill": {
               "duration": 0.011972,
               "end_time": "2025-05-16T08:45:14.867720",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.855748",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
            "    model.train()\n",
            "\n",
            "    total_loss = 0\n",
            "\n",
            "    for batch_idx, (data, target) in enumerate(train_loader):\n",
            "        data, target = data.to(device), target.to(device)\n",
            "\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
            "\n",
            "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
            "\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "        total_loss += loss.item()\n",
            "\n",
            "    return total_loss / len(train_loader)\n",
            "\n",
            "\n",
            "def evaluate(model, test_loader, criterion):\n",
            "    model.eval()\n",
            "\n",
            "    total_loss = 0\n",
            "    correct = 0\n",
            "\n",
            "    with torch.no_grad():\n",
            "        for batch_idx, (data, target) in enumerate(test_loader):\n",
            "            data, target = data.to(device), target.to(device)\n",
            "\n",
            "            output = model(data)\n",
            "\n",
            "            loss = criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(test_loader))\n",
            "            total_loss += loss.item()\n",
            "\n",
            "            preds = torch.argmax(output, 1)\n",
            "            correct += (preds == target).sum().item()\n",
            "\n",
            "            # print(f\"Predictions: {preds}, Targets: {target}\")\n",
            "\n",
            "    total = len(test_loader.dataset)\n",
            "    error = (total - correct) / total\n",
            "\n",
            "    print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
            "    return total_loss / total, error\n",
            "\n",
            "\n",
            "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
            "    for epoch in range(num_epochs):\n",
            "        now = time.time()\n",
            "\n",
            "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
            "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
            "\n",
            "        elapsed = time.time() - now\n",
            "\n",
            "        if use_wandb:\n",
            "            wandb.log({\n",
            "                \"epoch\": epoch,\n",
            "                \"train_loss\": train_loss,\n",
            "                \"val_loss\": val_loss,\n",
            "                \"val_error\": val_error\n",
            "            })\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.4f}, Time: {elapsed:.2f}s\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.873665Z",
               "iopub.status.busy": "2025-05-16T08:45:14.873450Z",
               "iopub.status.idle": "2025-05-16T08:45:14.877957Z",
               "shell.execute_reply": "2025-05-16T08:45:14.877481Z"
            },
            "papermill": {
               "duration": 0.008566,
               "end_time": "2025-05-16T08:45:14.878936",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.870370",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def train_mnist(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
            "    sigma1 = np.exp(-minus_log_sigma1)\n",
            "    sigma2 = np.exp(-minus_log_sigma2)\n",
            "\n",
            "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
            "    model.to(device)\n",
            "\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
            "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
            "\n",
            "    if use_wandb:\n",
            "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
            "\n",
            "    train(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
            "\n",
            "    if use_wandb:\n",
            "        run.finish()\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:14.884772Z",
               "iopub.status.busy": "2025-05-16T08:45:14.884574Z",
               "iopub.status.idle": "2025-05-16T08:45:20.023202Z",
               "shell.execute_reply": "2025-05-16T08:45:20.022489Z"
            },
            "papermill": {
               "duration": 5.14294,
               "end_time": "2025-05-16T08:45:20.024504",
               "exception": false,
               "start_time": "2025-05-16T08:45:14.881564",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "batch_size = 128\n",
            "transform = transforms.Compose([\n",
            "    transforms.ToImage(),\n",
            "    transforms.ToDtype(torch.float32, scale=True),\n",
            "    transforms.Normalize((0.1307,), (0.3081,)),\n",
            "    transforms.Lambda(lambda x: x.view(28 * 28)),\n",
            "])\n",
            "\n",
            "\n",
            "mnist_dataset = datasets.MNIST(\n",
            "    root=\"./mnist\",\n",
            "    download=True,\n",
            "    transform=transform,\n",
            "    train=True\n",
            ")\n",
            "# transformed_data = transform(mnist_dataset.data).to(device)\n",
            "# y = mnist_dataset.targets.to(device)\n",
            "# mnist_dataset = torch.utils.data.TensorDataset(transformed_data, y)\n",
            "\n",
            "\n",
            "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
            "\n",
            "kwargs = {\n",
            "    'batch_size': batch_size,\n",
            "    'num_workers': 2,\n",
            "    'generator': generator,\n",
            "}\n",
            "\n",
            "train_loader = torch.utils.data.DataLoader(\n",
            "    train_dataset,\n",
            "    shuffle=True,\n",
            "    **kwargs\n",
            ")\n",
            "val_loader = torch.utils.data.DataLoader(\n",
            "    val_dataset,\n",
            "    shuffle=False,\n",
            "    **kwargs\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "93e35068",
         "metadata": {
            "papermill": {
               "duration": 0.003165,
               "end_time": "2025-05-16T08:45:20.031268",
               "exception": false,
               "start_time": "2025-05-16T08:45:20.028103",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "## Grid search with wandb\n",
            "Uncomment the code below to run a grid search and log the results to wandb."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:20.038840Z",
               "iopub.status.busy": "2025-05-16T08:45:20.038640Z",
               "iopub.status.idle": "2025-05-16T08:45:20.042058Z",
               "shell.execute_reply": "2025-05-16T08:45:20.041578Z"
            },
            "papermill": {
               "duration": 0.008289,
               "end_time": "2025-05-16T08:45:20.043110",
               "exception": false,
               "start_time": "2025-05-16T08:45:20.034821",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# from kaggle_secrets import UserSecretsClient\n",
            "# user_secrets = UserSecretsClient()\n",
            "# key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "# wandb.login(key=key)\n",
            "\n",
            "\n",
            "# def train_wrapper():\n",
            "#     with wandb.init(project=\"asi-paper\") as run:\n",
            "#         model = train_mnist(\n",
            "#             train_loader,\n",
            "#             val_loader,\n",
            "#             epochs=10,\n",
            "#             lr=run.config.lr,\n",
            "#             num_samples=run.config.sample_nbr,\n",
            "#             pi=run.config.pi,\n",
            "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
            "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
            "#             use_wandb=True\n",
            "#         )\n",
            "\n",
            "#     return model\n",
            "\n",
            "\n",
            "# sweep_configuration = {\n",
            "#     \"method\": \"grid\",\n",
            "#     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n",
            "#     'name': \"sweep-mnist\",\n",
            "#     \"parameters\": {\n",
            "#         \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
            "#         \"sample_nbr\": {'values': [1, 2, 5, 10]},\n",
            "#         \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
            "#         \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
            "#         \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
            "#     },\n",
            "# }\n",
            "\n",
            "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
            "# wandb.agent(sweep_id, function=train_wrapper)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "5a03459f",
         "metadata": {
            "papermill": {
               "duration": 0.003023,
               "end_time": "2025-05-16T08:45:20.049393",
               "exception": false,
               "start_time": "2025-05-16T08:45:20.046370",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "## Manual training\n",
            "Uncomment the code below to train the model with specified hyperparameters and save the model checkpoint."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T08:45:20.057461Z",
               "iopub.status.busy": "2025-05-16T08:45:20.057182Z",
               "iopub.status.idle": "2025-05-16T11:19:04.119472Z",
               "shell.execute_reply": "2025-05-16T11:19:04.118851Z"
            },
            "papermill": {
               "duration": 9224.068009,
               "end_time": "2025-05-16T11:19:04.120895",
               "exception": false,
               "start_time": "2025-05-16T08:45:20.052886",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# from kaggle_secrets import UserSecretsClient\n",
            "# user_secrets = UserSecretsClient()\n",
            "# key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "# wandb.login(key=key)\n",
            "\n",
            "model = train_mnist(train_loader, val_loader, epochs=200, lr=1e-3, num_samples=1, pi=0.75, minus_log_sigma1=1, minus_log_sigma2=7, use_wandb=True)\n",
            "torch.save(model.state_dict(), \"mnist_model.pt\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-16T11:19:04.177268Z",
               "iopub.status.busy": "2025-05-16T11:19:04.177028Z",
               "iopub.status.idle": "2025-05-16T11:19:04.180641Z",
               "shell.execute_reply": "2025-05-16T11:19:04.180100Z"
            },
            "papermill": {
               "duration": 0.032192,
               "end_time": "2025-05-16T11:19:04.181662",
               "exception": false,
               "start_time": "2025-05-16T11:19:04.149470",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# model = MNISTModel(prior_sigma_1=np.exp(-1), prior_sigma_2=np.exp(-7), prior_pi=0.75)\n",
            "# model.to(device)\n",
            "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "1576c08f",
         "metadata": {
            "papermill": {
               "duration": 0.026577,
               "end_time": "2025-05-16T11:19:04.235071",
               "exception": false,
               "start_time": "2025-05-16T11:19:04.208494",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "# Regression curves"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def generate_samples(num_samples):\n",
            "    eps = np.random.normal(0, 0.02, num_samples)\n",
            "    x = np.linspace(0, 0.5, num_samples)\n",
            "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps))\n",
            "    return x, y\n",
            "\n",
            "\n",
            "def save_samples(x, y, filename):\n",
            "    df = pd.DataFrame({'x': x, 'y': y})\n",
            "    df.to_csv(filename, index=False)\n",
            "\n",
            "\n",
            "def load_samples(filename):\n",
            "    df = pd.read_csv(filename)\n",
            "    x = df['x'].values\n",
            "    y = df['y'].values\n",
            "    return x, y\n",
            "\n",
            "\n",
            "def plot_samples(x, y):\n",
            "    plt.figure(figsize=(10, 5))\n",
            "    plt.plot(x, y, 'kx', label='Generated Samples')\n",
            "    plt.title('Generated Samples')\n",
            "    plt.xlabel('x')\n",
            "    plt.ylabel('y')\n",
            "    plt.legend()\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "x, y = generate_samples(1000)\n",
            "save_samples(x, y, 'regression_samples.csv')\n",
            "\n",
            "\n",
            "x, y = load_samples('regression_samples.csv')\n",
            "plot_samples(x, y)\n",
            "\n",
            "X_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1).to(device)\n",
            "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
            "\n",
            "train_dataset = torch.utils.data.TensorDataset(X_tensor[:800], y_tensor[:800])\n",
            "val_dataset = torch.utils.data.TensorDataset(X_tensor[800:], y_tensor[800:])\n",
            "\n",
            "\n",
            "kwargs = {\n",
            "    'batch_size': batch_size,\n",
            "    'generator': generator,\n",
            "}\n",
            "\n",
            "train_loader = torch.utils.data.DataLoader(\n",
            "    train_dataset,\n",
            "    shuffle=True,\n",
            "    **kwargs\n",
            ")\n",
            "val_loader = torch.utils.data.DataLoader(\n",
            "    val_dataset,\n",
            "    shuffle=False,\n",
            "    **kwargs\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "@variational_estimator\n",
            "class RegressionModel(nn.Module):\n",
            "    def __init__(self, in_features=1, out_features=1, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
            "        super().__init__()\n",
            "\n",
            "        self.layers = nn.Sequential(\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                200,\n",
            "                200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                200,\n",
            "                out_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2,\n",
            "            ),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.layers(x)\n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def evaluate_regression(regressor, X, y, samples=100, std_multiplier=2):\n",
            "    preds = [regressor(X) for _ in range(samples)]\n",
            "    preds = torch.stack(preds)\n",
            "    means = preds.mean(axis=0)\n",
            "    stds = preds.std(axis=0)\n",
            "    ci_upper = means + (std_multiplier * stds)\n",
            "    ci_lower = means - (std_multiplier * stds)\n",
            "    ci_acc = (ci_lower <= y) * (ci_upper >= y)\n",
            "    ci_acc = ci_acc.float().mean()\n",
            "    return ci_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()\n",
            "\n",
            "\n",
            "def train_regression(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
            "    for epoch in range(num_epochs):\n",
            "        now = time.time()\n",
            "\n",
            "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
            "        ci_acc, ci_upper, ci_lower = evaluate_regression(model, val_loader.dataset.tensors[0], val_loader.dataset.tensors[1])\n",
            "\n",
            "        elapsed = time.time() - now\n",
            "\n",
            "        if use_wandb:\n",
            "            wandb.log({\n",
            "                \"epoch\": epoch,\n",
            "                \"train_loss\": train_loss,\n",
            "                \"ci_acc\": ci_acc,\n",
            "                \"ci_upper\": ci_upper,\n",
            "                \"ci_lower\": ci_lower,\n",
            "            })\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, CI acc: {ci_acc}, CI upper acc: {ci_upper}, CI lower acc: {ci_lower} Time: {elapsed:.2f}s\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def train_regression_model(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
            "    sigma1 = np.exp(-minus_log_sigma1)\n",
            "    sigma2 = np.exp(-minus_log_sigma2)\n",
            "\n",
            "    model = RegressionModel(1, 1, prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
            "    model.to(device)\n",
            "\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
            "    criterion = nn.MSELoss()\n",
            "\n",
            "    # if use_wandb:\n",
            "    #     run = wandb.init(project=\"asi-paper\", name=\"regression\")\n",
            "\n",
            "    train_regression(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
            "\n",
            "    # if use_wandb:\n",
            "    #     run.finish()\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "0408c1ea",
         "metadata": {},
         "outputs": [],
         "source": [
            "# from kaggle_secrets import UserSecretsClient\n",
            "# user_secrets = UserSecretsClient()\n",
            "# key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "# wandb.login(key=key)\n",
            "\n",
            "\n",
            "# def train_wrapper():\n",
            "#     with wandb.init(project=\"asi-paper\") as run:\n",
            "#         model = train_regression_model(\n",
            "#             train_loader,\n",
            "#             val_loader,\n",
            "#             epochs=15,\n",
            "#             lr=run.config.lr,\n",
            "#             num_samples=run.config.sample_nbr,\n",
            "#             pi=run.config.pi,\n",
            "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
            "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
            "#             use_wandb=True\n",
            "#         )\n",
            "\n",
            "#     return model\n",
            "\n",
            "\n",
            "# sweep_configuration = {\n",
            "#     \"method\": \"bayes\",\n",
            "#     \"metric\": {\"goal\": \"maximize\", \"name\": \"ci_acc\"},\n",
            "#     'name': \"sweep-regression\",\n",
            "#     \"parameters\": {\n",
            "#         \"lr\": {'min': 1e-5, 'max': 1e-2},\n",
            "#         \"sample_nbr\": {'min': 1, 'max': 10},\n",
            "#         \"pi\": {'min': 0.25, 'max': 0.75},\n",
            "#         \"min_log_sigma1\": {'min': 0, 'max': 2},\n",
            "#         \"min_log_sigma2\": {'min': 6, 'max': 8},\n",
            "#     },\n",
            "# }\n",
            "\n",
            "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
            "# wandb.agent(sweep_id, function=train_wrapper)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3c76d367",
         "metadata": {},
         "outputs": [],
         "source": [
            "# model = train_regression_model(train_loader, val_loader, epochs=10, lr=1e-3, num_samples=1, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "fbe0299e",
         "metadata": {},
         "outputs": [],
         "source": [
            "# model.eval()\n",
            "# predicted = model(X_tensor).cpu().detach().numpy()\n",
            "\n",
            "# plt.figure(figsize=(10, 5))\n",
            "# plt.plot(x, y, 'kx', label='Generated Samples')\n",
            "# plt.plot(x, predicted, 'r-', label='Predicted Mean')\n",
            "# # plt.fill_between(x, predicted - 2 * np.std(predicted), predicted + 2 * np.std(predicted), color='r', alpha=0.2, label='Uncertainty')\n",
            "# plt.title('Regression with Uncertainty')\n",
            "# plt.xlabel('x')\n",
            "# plt.ylabel('y')\n",
            "# plt.legend()\n",
            "# plt.show()"
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [],
         "dockerImageVersionId": 31040,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": "aml",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.13.3"
      },
      "papermill": {
         "default_parameters": {},
         "duration": 9250.710297,
         "end_time": "2025-05-16T11:19:07.419070",
         "environment_variables": {},
         "exception": null,
         "input_path": "__notebook__.ipynb",
         "output_path": "__notebook__.ipynb",
         "parameters": {},
         "start_time": "2025-05-16T08:44:56.708773",
         "version": "2.6.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}

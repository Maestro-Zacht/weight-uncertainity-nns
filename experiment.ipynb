{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": 1,
         "id": "e93c694c",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:05.531482Z",
               "iopub.status.busy": "2025-05-18T09:14:05.531217Z",
               "iopub.status.idle": "2025-05-18T09:14:19.573295Z",
               "shell.execute_reply": "2025-05-18T09:14:19.572666Z"
            },
            "papermill": {
               "duration": 14.048355,
               "end_time": "2025-05-18T09:14:19.574629",
               "exception": false,
               "start_time": "2025-05-18T09:14:05.526274",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "from typing import Optional\n",
            "import torch\n",
            "from torch import nn\n",
            "from torch.nn import functional as F\n",
            "from torchvision import datasets\n",
            "from torchvision.transforms import v2 as transforms\n",
            "import numpy as np\n",
            "import wandb\n",
            "import time\n",
            "import matplotlib.pyplot as plt\n",
            "import pandas as pd"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "id": "4e0e8ed5",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.582410Z",
               "iopub.status.busy": "2025-05-18T09:14:19.581782Z",
               "iopub.status.idle": "2025-05-18T09:14:19.585362Z",
               "shell.execute_reply": "2025-05-18T09:14:19.584640Z"
            },
            "papermill": {
               "duration": 0.00832,
               "end_time": "2025-05-18T09:14:19.586495",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.578175",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "generator = torch.Generator().manual_seed(42)\n",
            "np.random.seed(42)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "id": "e01c1ecd",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.615051Z",
               "iopub.status.busy": "2025-05-18T09:14:19.614848Z",
               "iopub.status.idle": "2025-05-18T09:14:19.682904Z",
               "shell.execute_reply": "2025-05-18T09:14:19.682183Z"
            },
            "papermill": {
               "duration": 0.072543,
               "end_time": "2025-05-18T09:14:19.683925",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.611382",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Using device: cuda\n"
               ]
            }
         ],
         "source": [
            "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
            "print(f\"Using device: {device}\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "id": "96ee072c",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.593596Z",
               "iopub.status.busy": "2025-05-18T09:14:19.593386Z",
               "iopub.status.idle": "2025-05-18T09:14:19.607312Z",
               "shell.execute_reply": "2025-05-18T09:14:19.606819Z"
            },
            "papermill": {
               "duration": 0.018665,
               "end_time": "2025-05-18T09:14:19.608315",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.589650",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "\n",
            "class GaussianPosterior(nn.Module):\n",
            "    LOG_SQRT_2PI = 0.5 * np.log(2 * np.pi)\n",
            "\n",
            "    def __init__(self, mu, rho):\n",
            "        super().__init__()\n",
            "\n",
            "        self.mu = nn.Parameter(mu)\n",
            "        self.rho = nn.Parameter(rho)\n",
            "\n",
            "        self.w = None\n",
            "        self.sigma = None\n",
            "\n",
            "        self.normal = torch.distributions.Normal(0, 1)\n",
            "\n",
            "    def sample(self):\n",
            "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
            "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
            "        self.w = self.mu + self.sigma * epsilon\n",
            "\n",
            "        return self.w\n",
            "\n",
            "    def log_posterior(self):\n",
            "        assert self.w is not None\n",
            "        assert self.sigma is not None\n",
            "\n",
            "        log_posterior = -GaussianPosterior.LOG_SQRT_2PI - torch.log(self.sigma) - ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
            "\n",
            "        return log_posterior.sum()\n",
            "\n",
            "\n",
            "class ScaleMixturePrior(nn.Module):\n",
            "\n",
            "    def __init__(self, pi: float, sigma1: float, sigma2: float):\n",
            "        super().__init__()\n",
            "\n",
            "        self.pi = pi\n",
            "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
            "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
            "\n",
            "    def log_prior(self, w):\n",
            "        likelihood1 = torch.exp(self.normal1.log_prob(w))\n",
            "        likelihood2 = torch.exp(self.normal2.log_prob(w))\n",
            "\n",
            "        p_mixture = self.pi * likelihood1 + (1 - self.pi) * likelihood2\n",
            "        log_prob = torch.log(p_mixture).sum()\n",
            "\n",
            "        return log_prob\n",
            "\n",
            "\n",
            "class BayesianModule(nn.Module):\n",
            "    pass\n",
            "\n",
            "\n",
            "class BayesLinear(BayesianModule):\n",
            "\n",
            "    def __init__(\n",
            "        self,\n",
            "            in_features: int,\n",
            "            out_features: int,\n",
            "            prior_pi: float,\n",
            "            prior_sigma1: float,\n",
            "            prior_sigma2: float\n",
            "    ):\n",
            "        super().__init__()\n",
            "\n",
            "        w_mu = torch.empty(out_features, in_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
            "        w_rho = torch.empty(out_features, in_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
            "\n",
            "        bias_mu = torch.empty(out_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
            "        bias_rho = torch.empty(out_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
            "\n",
            "        self.w_posterior = GaussianPosterior(w_mu, w_rho)\n",
            "        self.b_posterior = GaussianPosterior(bias_mu, bias_rho)\n",
            "\n",
            "        self.w_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
            "        self.b_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
            "\n",
            "    def forward(self, x):\n",
            "        w = self.w_posterior.sample()\n",
            "        b = self.b_posterior.sample()\n",
            "\n",
            "        log_prior = self.w_prior.log_prior(w) + self.b_prior.log_prior(b)\n",
            "        log_posterior = self.w_posterior.log_posterior() + self.b_posterior.log_posterior()\n",
            "\n",
            "        self.kl_divergence = log_posterior - log_prior\n",
            "\n",
            "        return F.linear(x, w, b)\n",
            "\n",
            "\n",
            "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
            "    return 1 / num_batches"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "11cb55c1",
         "metadata": {
            "papermill": {
               "duration": 0.002852,
               "end_time": "2025-05-18T09:14:19.690025",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.687173",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "# MNIST classification"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "id": "7f9df508",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.697023Z",
               "iopub.status.busy": "2025-05-18T09:14:19.696768Z",
               "iopub.status.idle": "2025-05-18T09:14:19.701757Z",
               "shell.execute_reply": "2025-05-18T09:14:19.701059Z"
            },
            "papermill": {
               "duration": 0.009841,
               "end_time": "2025-05-18T09:14:19.702823",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.692982",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "class MNISTModel(nn.Module):\n",
            "\n",
            "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
            "        super().__init__()\n",
            "\n",
            "        self.layers = nn.Sequential(\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                in_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                in_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                out_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2,\n",
            "            ),\n",
            "            nn.Softmax(dim=1),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.layers(x)\n",
            "        # print(x)\n",
            "        return x\n",
            "\n",
            "    @property\n",
            "    def kl_divergence(self):\n",
            "        kl = 0\n",
            "        for module in self.modules():\n",
            "            if isinstance(module, BayesianModule):\n",
            "                kl += module.kl_divergence\n",
            "\n",
            "        return kl\n",
            "\n",
            "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
            "        loss = 0\n",
            "        for _ in range(num_samples):\n",
            "            outputs = self(inputs)\n",
            "            contr1 = criterion(outputs, labels)\n",
            "            contr2 = self.kl_divergence * complexity_cost_weight\n",
            "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
            "            loss += contr1 + contr2\n",
            "        return loss / num_samples"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "id": "8801c1a1",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.709645Z",
               "iopub.status.busy": "2025-05-18T09:14:19.709444Z",
               "iopub.status.idle": "2025-05-18T09:14:19.716950Z",
               "shell.execute_reply": "2025-05-18T09:14:19.716399Z"
            },
            "papermill": {
               "duration": 0.012104,
               "end_time": "2025-05-18T09:14:19.717991",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.705887",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
            "    model.train()\n",
            "\n",
            "    total_loss = 0\n",
            "\n",
            "    for batch_idx, (data, target) in enumerate(train_loader):\n",
            "        data, target = data.to(device), target.to(device)\n",
            "\n",
            "        optimizer.zero_grad()\n",
            "\n",
            "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
            "\n",
            "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
            "\n",
            "        loss.backward()\n",
            "        optimizer.step()\n",
            "\n",
            "        total_loss += loss.item()\n",
            "\n",
            "    return total_loss / len(train_loader)\n",
            "\n",
            "\n",
            "def evaluate(model, val_loader, criterion):\n",
            "    model.eval()\n",
            "\n",
            "    total_loss = 0\n",
            "    correct = 0\n",
            "\n",
            "    with torch.no_grad():\n",
            "        for batch_idx, (data, target) in enumerate(val_loader):\n",
            "            data, target = data.to(device), target.to(device)\n",
            "\n",
            "            output = model(data)\n",
            "\n",
            "            preds = torch.argmax(output, 1)\n",
            "            correct += (preds == target).sum().item()\n",
            "\n",
            "            loss = (\n",
            "                criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(val_loader))\n",
            "            )\n",
            "            total_loss += loss.item()\n",
            "\n",
            "    total = len(val_loader.dataset)\n",
            "    return total_loss / total, (total - correct) / total\n",
            "\n",
            "\n",
            "def test(model, test_loader):\n",
            "    model.eval()\n",
            "\n",
            "    correct = 0\n",
            "\n",
            "    with torch.no_grad():\n",
            "        for data, target in test_loader:\n",
            "            data, target = data.to(device), target.to(device)\n",
            "\n",
            "            output = model(data)\n",
            "\n",
            "            preds = torch.argmax(output, 1)\n",
            "            correct += (preds == target).sum().item()\n",
            "\n",
            "    total = len(test_loader.dataset)\n",
            "    error = (total - correct) / total\n",
            "\n",
            "    # print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
            "    return error\n",
            "\n",
            "\n",
            "def train(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
            "    for epoch in range(num_epochs):\n",
            "        now = time.time()\n",
            "\n",
            "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
            "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
            "        test_error = test(model, test_loader)\n",
            "\n",
            "        elapsed = time.time() - now\n",
            "\n",
            "        if use_wandb:\n",
            "            wandb.log({\n",
            "                \"epoch\": epoch,\n",
            "                \"train_loss\": train_loss,\n",
            "                \"val_loss\": val_loss,\n",
            "                \"val_error\": val_error,\n",
            "                \"test_error\": test_error\n",
            "            })\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.2%}, Test Error: {test_error:.2%}, Time: {elapsed:.2f}s\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "id": "c87e7ebb",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.724639Z",
               "iopub.status.busy": "2025-05-18T09:14:19.724446Z",
               "iopub.status.idle": "2025-05-18T09:14:19.728906Z",
               "shell.execute_reply": "2025-05-18T09:14:19.728346Z"
            },
            "papermill": {
               "duration": 0.008846,
               "end_time": "2025-05-18T09:14:19.729859",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.721013",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def train_mnist(train_loader, val_loader, test_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
            "    sigma1 = np.exp(-minus_log_sigma1)\n",
            "    sigma2 = np.exp(-minus_log_sigma2)\n",
            "\n",
            "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
            "    model.to(device)\n",
            "\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
            "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
            "\n",
            "    if use_wandb:\n",
            "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
            "\n",
            "    train(model, train_loader, val_loader, test_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
            "\n",
            "    if use_wandb:\n",
            "        run.finish()\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "id": "955153f5",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:19.736643Z",
               "iopub.status.busy": "2025-05-18T09:14:19.736445Z",
               "iopub.status.idle": "2025-05-18T09:14:24.397298Z",
               "shell.execute_reply": "2025-05-18T09:14:24.396351Z"
            },
            "papermill": {
               "duration": 4.665538,
               "end_time": "2025-05-18T09:14:24.398487",
               "exception": false,
               "start_time": "2025-05-18T09:14:19.732949",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "batch_size = 128\n",
            "transform = transforms.Compose([\n",
            "    transforms.ToImage(),\n",
            "    transforms.ToDtype(torch.float32, scale=True),\n",
            "    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n",
            "])\n",
            "\n",
            "\n",
            "mnist_dataset = datasets.MNIST(\n",
            "    root=\"./mnist\",\n",
            "    download=True,\n",
            "    transform=transform,\n",
            "    train=True\n",
            ")\n",
            "# transformed_data = transform(mnist_dataset.data).to(device)\n",
            "# y = mnist_dataset.targets.to(device)\n",
            "# mnist_dataset = torch.utils.data.TensorDataset(transformed_data, y)\n",
            "\n",
            "test_set = datasets.MNIST(\n",
            "    root=\"./mnist\",\n",
            "    download=True,\n",
            "    transform=transform,\n",
            "    train=False\n",
            ")\n",
            "\n",
            "\n",
            "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
            "\n",
            "kwargs = {\n",
            "    'batch_size': batch_size,\n",
            "    'num_workers': 1,\n",
            "    'generator': generator,\n",
            "    'pin_memory': True,\n",
            "}\n",
            "\n",
            "train_loader = torch.utils.data.DataLoader(\n",
            "    train_dataset,\n",
            "    shuffle=True,\n",
            "    **kwargs\n",
            ")\n",
            "val_loader = torch.utils.data.DataLoader(\n",
            "    val_dataset,\n",
            "    shuffle=False,\n",
            "    **kwargs\n",
            ")\n",
            "test_loader = torch.utils.data.DataLoader(\n",
            "    test_set,\n",
            "    shuffle=False,\n",
            "    **kwargs\n",
            ")"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "8fcbe99a",
         "metadata": {
            "papermill": {
               "duration": 0.003816,
               "end_time": "2025-05-18T09:14:24.406597",
               "exception": false,
               "start_time": "2025-05-18T09:14:24.402781",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "## Grid search with wandb\n",
            "Uncomment the code below to run a grid search and log the results to wandb."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "ba3cd36d",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:24.415356Z",
               "iopub.status.busy": "2025-05-18T09:14:24.414916Z",
               "iopub.status.idle": "2025-05-18T09:14:24.418147Z",
               "shell.execute_reply": "2025-05-18T09:14:24.417651Z"
            },
            "papermill": {
               "duration": 0.008688,
               "end_time": "2025-05-18T09:14:24.419128",
               "exception": false,
               "start_time": "2025-05-18T09:14:24.410440",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "from kaggle_secrets import UserSecretsClient\n",
            "user_secrets = UserSecretsClient()\n",
            "key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "wandb.login(key=key)\n",
            "\n",
            "\n",
            "def train_wrapper():\n",
            "    with wandb.init(project=\"asi-paper\") as run:\n",
            "        model = train_mnist(\n",
            "            train_loader,\n",
            "            val_loader,\n",
            "            test_loader,\n",
            "            epochs=10,\n",
            "            lr=run.config.lr,\n",
            "            num_samples=run.config.sample_nbr,\n",
            "            pi=run.config.pi,\n",
            "            minus_log_sigma1=run.config.min_log_sigma1,\n",
            "            minus_log_sigma2=run.config.min_log_sigma2,\n",
            "            use_wandb=True\n",
            "        )\n",
            "\n",
            "    return model\n",
            "\n",
            "\n",
            "sweep_configuration = {\n",
            "    \"method\": \"grid\",\n",
            "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n",
            "    'name': \"sweep-mnist\",\n",
            "    \"parameters\": {\n",
            "        \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
            "        \"sample_nbr\": {'values': [1, 2, 3, 5]},\n",
            "        \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
            "        \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
            "        \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
            "    },\n",
            "}\n",
            "\n",
            "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
            "print(f\"Sweep ID: {sweep_id}\")\n",
            "wandb.agent(sweep_id, function=train_wrapper)"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "2df0eb61",
         "metadata": {
            "papermill": {
               "duration": 0.003612,
               "end_time": "2025-05-18T09:14:24.426625",
               "exception": false,
               "start_time": "2025-05-18T09:14:24.423013",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "## Manual training\n",
            "Uncomment the code below to train the model with specified hyperparameters and save it."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "22e2af5b",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T09:14:24.434954Z",
               "iopub.status.busy": "2025-05-18T09:14:24.434770Z",
               "iopub.status.idle": "2025-05-18T10:24:20.330538Z",
               "shell.execute_reply": "2025-05-18T10:24:20.329903Z"
            },
            "papermill": {
               "duration": 4195.901559,
               "end_time": "2025-05-18T10:24:20.331982",
               "exception": false,
               "start_time": "2025-05-18T09:14:24.430423",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Epoch 1/10, Train Loss: 11319.2300, Val Loss: 371.9498, Val Error: 18.94%, Test Error: 17.36%, Time: 9.66s\n"
               ]
            },
            {
               "ename": "KeyboardInterrupt",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                  "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# from kaggle_secrets import UserSecretsClient\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# user_secrets = UserSecretsClient()\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# key = user_secrets.get_secret('wand-api-key-asi')\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# wandb.login(key=key)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m model = \u001b[43mtrain_mnist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_log_sigma1\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminus_log_sigma2\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# torch.save(model.state_dict(), \"mnist_model.pt\")\u001b[39;00m\n",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mtrain_mnist\u001b[39m\u001b[34m(train_loader, val_loader, test_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n\u001b[32m     12\u001b[39m     run = wandb.init(project=\u001b[33m\"\u001b[39m\u001b[33masi-paper\u001b[39m\u001b[33m\"\u001b[39m, name=\u001b[33m\"\u001b[39m\u001b[33mmnist\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_wandb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_wandb:\n\u001b[32m     17\u001b[39m     run.finish()\n",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs, num_samples, use_wandb)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m     70\u001b[39m     now = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m     val_loss, val_error = evaluate(model, val_loader, criterion)\n\u001b[32m     74\u001b[39m     test_error = test(model, test_loader)\n",
                  "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, num_samples)\u001b[39m\n\u001b[32m     13\u001b[39m     loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n\u001b[32m     15\u001b[39m     loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     total_loss += loss.item()\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    496\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     89\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     93\u001b[39m     torch._dynamo.graph_break()\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/adam.py:244\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    232\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    234\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    235\u001b[39m         group,\n\u001b[32m    236\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    241\u001b[39m         state_steps,\n\u001b[32m    242\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    246\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/adam.py:876\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    873\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    874\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m876\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    894\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
                  "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/aml/lib/python3.13/site-packages/torch/optim/adam.py:619\u001b[39m, in \u001b[36m_multi_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[39m\n\u001b[32m    612\u001b[39m         device_grads = torch._foreach_add(  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[32m    613\u001b[39m             device_grads, device_params, alpha=weight_decay\n\u001b[32m    614\u001b[39m         )\n\u001b[32m    616\u001b[39m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[32m    617\u001b[39m \u001b[38;5;66;03m# Use device beta1 if beta1 is a tensor to ensure all\u001b[39;00m\n\u001b[32m    618\u001b[39m \u001b[38;5;66;03m# tensors are on the same device\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_lerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_exp_avgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_grads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_beta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m torch._foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[32m    623\u001b[39m \u001b[38;5;66;03m# Due to the strictness of the _foreach_addcmul API, we can't have a single\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;66;03m# tensor scalar as the scalar arg (only python number is supported there)\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;66;03m# as a result, separate out the value mul\u001b[39;00m\n\u001b[32m    626\u001b[39m \u001b[38;5;66;03m# Filed https://github.com/pytorch/pytorch/issues/139795\u001b[39;00m\n",
                  "\u001b[31mKeyboardInterrupt\u001b[39m: "
               ]
            }
         ],
         "source": [
            "# from kaggle_secrets import UserSecretsClient\n",
            "# user_secrets = UserSecretsClient()\n",
            "# key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "# wandb.login(key=key)\n",
            "\n",
            "# model = train_mnist(train_loader, val_loader, test_loader, epochs=10, lr=1e-3, num_samples=1, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6, use_wandb=False)\n",
            "# torch.save(model.state_dict(), \"mnist_model.pt\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "cba3dbf0",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.357909Z",
               "iopub.status.busy": "2025-05-18T10:24:20.357557Z",
               "iopub.status.idle": "2025-05-18T10:24:20.361079Z",
               "shell.execute_reply": "2025-05-18T10:24:20.360555Z"
            },
            "papermill": {
               "duration": 0.017539,
               "end_time": "2025-05-18T10:24:20.362116",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.344577",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# model = MNISTModel(prior_sigma_1=np.exp(-1), prior_sigma_2=np.exp(-7), prior_pi=0.75)\n",
            "# model.to(device)\n",
            "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
         ]
      },
      {
         "cell_type": "markdown",
         "id": "05676d69",
         "metadata": {
            "papermill": {
               "duration": 0.011819,
               "end_time": "2025-05-18T10:24:20.385892",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.374073",
               "status": "completed"
            },
            "tags": []
         },
         "source": [
            "# Regression curves"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "3f8515f2",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.410704Z",
               "iopub.status.busy": "2025-05-18T10:24:20.410477Z",
               "iopub.status.idle": "2025-05-18T10:24:20.695717Z",
               "shell.execute_reply": "2025-05-18T10:24:20.695169Z"
            },
            "papermill": {
               "duration": 0.298933,
               "end_time": "2025-05-18T10:24:20.696829",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.397896",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def generate_samples(num_samples):\n",
            "    eps = np.random.normal(0, 0.02, num_samples)\n",
            "    x = np.linspace(0, 0.5, num_samples)\n",
            "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps))\n",
            "    return x, y\n",
            "\n",
            "\n",
            "def save_samples(x, y, filename):\n",
            "    df = pd.DataFrame({'x': x, 'y': y})\n",
            "    df.to_csv(filename, index=False)\n",
            "\n",
            "\n",
            "def load_samples(filename):\n",
            "    df = pd.read_csv(filename)\n",
            "    x = df['x'].values\n",
            "    y = df['y'].values\n",
            "    return x, y\n",
            "\n",
            "\n",
            "def plot_samples(x, y):\n",
            "    plt.figure(figsize=(10, 5))\n",
            "    plt.plot(x, y, 'kx', label='Generated Samples')\n",
            "    plt.title('Generated Samples')\n",
            "    plt.xlabel('x')\n",
            "    plt.ylabel('y')\n",
            "    plt.legend()\n",
            "    plt.show()\n",
            "\n",
            "\n",
            "# x, y = generate_samples(1000)\n",
            "# save_samples(x, y, 'regression_samples.csv')\n",
            "\n",
            "\n",
            "# x, y = load_samples('regression_samples.csv')\n",
            "# plot_samples(x, y)\n",
            "\n",
            "# X_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1).to(device)\n",
            "# y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
            "\n",
            "# train_dataset = torch.utils.data.TensorDataset(X_tensor[:800], y_tensor[:800])\n",
            "# val_dataset = torch.utils.data.TensorDataset(X_tensor[800:], y_tensor[800:])\n",
            "\n",
            "\n",
            "# kwargs = {\n",
            "#     'batch_size': batch_size,\n",
            "#     'generator': generator,\n",
            "# }\n",
            "\n",
            "# train_loader = torch.utils.data.DataLoader(\n",
            "#     train_dataset,\n",
            "#     shuffle=True,\n",
            "#     **kwargs\n",
            "# )\n",
            "# val_loader = torch.utils.data.DataLoader(\n",
            "#     val_dataset,\n",
            "#     shuffle=False,\n",
            "#     **kwargs\n",
            "# )"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "a66e7c22",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.723766Z",
               "iopub.status.busy": "2025-05-18T10:24:20.723513Z",
               "iopub.status.idle": "2025-05-18T10:24:20.728128Z",
               "shell.execute_reply": "2025-05-18T10:24:20.727616Z"
            },
            "papermill": {
               "duration": 0.019103,
               "end_time": "2025-05-18T10:24:20.729133",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.710030",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "class RegressionModel(nn.Module):\n",
            "    def __init__(self, in_features=1, out_features=1, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
            "        super().__init__()\n",
            "\n",
            "        self.layers = nn.Sequential(\n",
            "            BayesLinear(\n",
            "                in_features,\n",
            "                200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                200,\n",
            "                200,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2\n",
            "            ),\n",
            "            nn.ReLU(),\n",
            "            BayesLinear(\n",
            "                200,\n",
            "                out_features,\n",
            "                prior_pi,\n",
            "                prior_sigma_1,\n",
            "                prior_sigma_2,\n",
            "            ),\n",
            "        )\n",
            "\n",
            "    def forward(self, x):\n",
            "        x = self.layers(x)\n",
            "        return x"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "9c6ad1b4",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.755128Z",
               "iopub.status.busy": "2025-05-18T10:24:20.754923Z",
               "iopub.status.idle": "2025-05-18T10:24:20.760930Z",
               "shell.execute_reply": "2025-05-18T10:24:20.760399Z"
            },
            "papermill": {
               "duration": 0.020355,
               "end_time": "2025-05-18T10:24:20.761940",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.741585",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def evaluate_regression(regressor, X, y, samples=100, std_multiplier=2):\n",
            "    preds = [regressor(X) for _ in range(samples)]\n",
            "    preds = torch.stack(preds)\n",
            "    means = preds.mean(axis=0)\n",
            "    stds = preds.std(axis=0)\n",
            "    ci_upper = means + (std_multiplier * stds)\n",
            "    ci_lower = means - (std_multiplier * stds)\n",
            "    ci_acc = (ci_lower <= y) * (ci_upper >= y)\n",
            "    ci_acc = ci_acc.float().mean()\n",
            "    return ci_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()\n",
            "\n",
            "\n",
            "def train_regression(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
            "    for epoch in range(num_epochs):\n",
            "        now = time.time()\n",
            "\n",
            "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
            "        ci_acc, ci_upper, ci_lower = evaluate_regression(model, val_loader.dataset.tensors[0], val_loader.dataset.tensors[1])\n",
            "\n",
            "        elapsed = time.time() - now\n",
            "\n",
            "        if use_wandb:\n",
            "            wandb.log({\n",
            "                \"epoch\": epoch,\n",
            "                \"train_loss\": train_loss,\n",
            "                \"ci_acc\": ci_acc,\n",
            "                \"ci_upper\": ci_upper,\n",
            "                \"ci_lower\": ci_lower,\n",
            "            })\n",
            "\n",
            "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, CI acc: {ci_acc}, CI upper acc: {ci_upper}, CI lower acc: {ci_lower} Time: {elapsed:.2f}s\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d8690703",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.788127Z",
               "iopub.status.busy": "2025-05-18T10:24:20.787911Z",
               "iopub.status.idle": "2025-05-18T10:24:20.792244Z",
               "shell.execute_reply": "2025-05-18T10:24:20.791703Z"
            },
            "papermill": {
               "duration": 0.01878,
               "end_time": "2025-05-18T10:24:20.793287",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.774507",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "def train_regression_model(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
            "    sigma1 = np.exp(-minus_log_sigma1)\n",
            "    sigma2 = np.exp(-minus_log_sigma2)\n",
            "\n",
            "    model = RegressionModel(1, 1, prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
            "    model.to(device)\n",
            "\n",
            "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
            "    criterion = nn.MSELoss()\n",
            "\n",
            "    # if use_wandb:\n",
            "    #     run = wandb.init(project=\"asi-paper\", name=\"regression\")\n",
            "\n",
            "    train_regression(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
            "\n",
            "    # if use_wandb:\n",
            "    #     run.finish()\n",
            "\n",
            "    return model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "d4535a08",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.819452Z",
               "iopub.status.busy": "2025-05-18T10:24:20.819255Z",
               "iopub.status.idle": "2025-05-18T10:24:20.822730Z",
               "shell.execute_reply": "2025-05-18T10:24:20.822065Z"
            },
            "papermill": {
               "duration": 0.018148,
               "end_time": "2025-05-18T10:24:20.823933",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.805785",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# from kaggle_secrets import UserSecretsClient\n",
            "# user_secrets = UserSecretsClient()\n",
            "# key = user_secrets.get_secret('wand-api-key-asi')\n",
            "\n",
            "# wandb.login(key=key)\n",
            "\n",
            "\n",
            "# def train_wrapper():\n",
            "#     with wandb.init(project=\"asi-paper\") as run:\n",
            "#         model = train_regression_model(\n",
            "#             train_loader,\n",
            "#             val_loader,\n",
            "#             epochs=15,\n",
            "#             lr=run.config.lr,\n",
            "#             num_samples=run.config.sample_nbr,\n",
            "#             pi=run.config.pi,\n",
            "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
            "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
            "#             use_wandb=True\n",
            "#         )\n",
            "\n",
            "#     return model\n",
            "\n",
            "\n",
            "# sweep_configuration = {\n",
            "#     \"method\": \"bayes\",\n",
            "#     \"metric\": {\"goal\": \"maximize\", \"name\": \"ci_acc\"},\n",
            "#     'name': \"sweep-regression\",\n",
            "#     \"parameters\": {\n",
            "#         \"lr\": {'min': 1e-5, 'max': 1e-2},\n",
            "#         \"sample_nbr\": {'min': 1, 'max': 10},\n",
            "#         \"pi\": {'min': 0.25, 'max': 0.75},\n",
            "#         \"min_log_sigma1\": {'min': 0, 'max': 2},\n",
            "#         \"min_log_sigma2\": {'min': 6, 'max': 8},\n",
            "#     },\n",
            "# }\n",
            "\n",
            "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
            "# wandb.agent(sweep_id, function=train_wrapper)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "c2a195d4",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.850597Z",
               "iopub.status.busy": "2025-05-18T10:24:20.850098Z",
               "iopub.status.idle": "2025-05-18T10:24:20.853205Z",
               "shell.execute_reply": "2025-05-18T10:24:20.852652Z"
            },
            "papermill": {
               "duration": 0.017372,
               "end_time": "2025-05-18T10:24:20.854217",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.836845",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# model = train_regression_model(train_loader, val_loader, epochs=10, lr=1e-3, num_samples=1, pi=0.5, minus_log_sigma1=0.5, minus_log_sigma2=0)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "id": "76dde1bb",
         "metadata": {
            "execution": {
               "iopub.execute_input": "2025-05-18T10:24:20.880401Z",
               "iopub.status.busy": "2025-05-18T10:24:20.880199Z",
               "iopub.status.idle": "2025-05-18T10:24:20.883325Z",
               "shell.execute_reply": "2025-05-18T10:24:20.882648Z"
            },
            "papermill": {
               "duration": 0.017291,
               "end_time": "2025-05-18T10:24:20.884334",
               "exception": false,
               "start_time": "2025-05-18T10:24:20.867043",
               "status": "completed"
            },
            "tags": []
         },
         "outputs": [],
         "source": [
            "# model.eval()\n",
            "# predicted = model(X_tensor).cpu().detach().numpy()\n",
            "\n",
            "# plt.figure(figsize=(10, 5))\n",
            "# plt.plot(x, y, 'kx', label='Generated Samples')\n",
            "# plt.plot(x, predicted, 'r-', label='Predicted Mean')\n",
            "# # plt.fill_between(x, predicted - 2 * np.std(predicted), predicted + 2 * np.std(predicted), color='r', alpha=0.2, label='Uncertainty')\n",
            "# plt.title('Regression with Uncertainty')\n",
            "# plt.xlabel('x')\n",
            "# plt.ylabel('y')\n",
            "# plt.legend()\n",
            "# plt.show()"
         ]
      }
   ],
   "metadata": {
      "kaggle": {
         "accelerator": "gpu",
         "dataSources": [],
         "dockerImageVersionId": 31040,
         "isGpuEnabled": true,
         "isInternetEnabled": true,
         "language": "python",
         "sourceType": "notebook"
      },
      "kernelspec": {
         "display_name": "aml",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.13.3"
      },
      "papermill": {
         "default_parameters": {},
         "duration": 4223.060462,
         "end_time": "2025-05-18T10:24:24.455895",
         "environment_variables": {},
         "exception": null,
         "input_path": "__notebook__.ipynb",
         "output_path": "__notebook__.ipynb",
         "parameters": {},
         "start_time": "2025-05-18T09:14:01.395433",
         "version": "2.6.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 5
}

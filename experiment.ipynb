{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dbd8c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71fda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainableNormalDistribution(nn.Module):\n",
    "    LOG_SQRT2PI = np.log(np.sqrt(2 * np.pi))\n",
    "\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "        self.register_buffer('eps', torch.Tensor(self.mu.shape))\n",
    "        self.sigma = None\n",
    "        self.w = None\n",
    "\n",
    "    def sample(self):\n",
    "        self.eps.data.normal_()\n",
    "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
    "        self.w = self.mu + self.sigma * self.eps\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self):\n",
    "        assert (self.w is not None), \"You can only have a log posterior for W if you've already sampled it\"\n",
    "\n",
    "        log_posteriors = -TrainableNormalDistribution.LOG_SQRT2PI - torch.log(self.sigma) - (((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)) - 0.5\n",
    "        return log_posteriors.sum()\n",
    "\n",
    "\n",
    "class PriorWeightDistribution(nn.Module):\n",
    "    # Calculates a Scale Mixture Prior distribution for the prior part of the complexity cost on Bayes by Backprop paper\n",
    "    def __init__(self, pi, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.dist1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.dist2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w):\n",
    "        prob_n1 = torch.exp(self.dist1.log_prob(w))\n",
    "        prob_n2 = torch.exp(self.dist2.log_prob(w))\n",
    "\n",
    "        # Prior of the mixture distribution, adding 1e-6 prevents numeric problems with log(p) for small p\n",
    "        prior_pdf = (self.pi * prob_n1 + (1 - self.pi) * prob_n2) + 1e-6\n",
    "\n",
    "        return (torch.log(prior_pdf) - 0.5).sum()\n",
    "\n",
    "\n",
    "class BayesianLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0, prior_dist=None):\n",
    "        super().__init__()\n",
    "\n",
    "        # our main parameters\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.bias = bias\n",
    "\n",
    "        # parameters for the scale mixture prior\n",
    "        self.prior_sigma_1 = prior_sigma_1\n",
    "        self.prior_sigma_2 = prior_sigma_2\n",
    "        self.prior_pi = prior_pi\n",
    "        self.prior_dist = prior_dist\n",
    "\n",
    "        # Variational weight parameters and sample\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_mu_init, 0.1))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_rho_init, 0.1))\n",
    "        self.weight_sampler = TrainableNormalDistribution(self.weight_mu, self.weight_rho)\n",
    "\n",
    "        # Variational bias parameters and sample\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(posterior_mu_init, 0.1))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(posterior_rho_init, 0.1))\n",
    "        self.bias_sampler = TrainableNormalDistribution(self.bias_mu, self.bias_rho)\n",
    "\n",
    "        # Priors (as BBP paper)\n",
    "        self.weight_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n",
    "        self.bias_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n",
    "        self.log_prior = 0\n",
    "        self.log_variational_posterior = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Sample the weights and forward it\n",
    "        w = self.weight_sampler.sample()\n",
    "\n",
    "        if self.bias:\n",
    "            b = self.bias_sampler.sample()\n",
    "            b_log_posterior = self.bias_sampler.log_posterior()\n",
    "            b_log_prior = self.bias_prior_dist.log_prior(b)\n",
    "        else:\n",
    "            b = torch.zeros((self.out_features), device=x.device)\n",
    "            b_log_posterior = 0\n",
    "            b_log_prior = 0\n",
    "\n",
    "        # Get the complexity cost\n",
    "        self.log_variational_posterior = self.weight_sampler.log_posterior() + b_log_posterior\n",
    "        self.log_prior = self.weight_prior_dist.log_prior(w) + b_log_prior\n",
    "\n",
    "        # print(x.shape, w.shape, b.shape)\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        return self.log_variational_posterior - self.log_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75be44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesianLinear(\n",
    "                in_features, in_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(\n",
    "                in_features, in_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesianLinear(\n",
    "                in_features, out_features,\n",
    "                prior_sigma_1=prior_sigma_1,\n",
    "                prior_sigma_2=prior_sigma_2,\n",
    "                prior_pi=prior_pi,\n",
    "                posterior_mu_init=posterior_mu_init,\n",
    "                posterior_rho_init=posterior_rho_init\n",
    "            ),\n",
    "            nn.Softmax(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            kl += getattr(module, 'kl_divergence', 0) if module != self else 0\n",
    "        return kl\n",
    "\n",
    "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
    "        loss = 0\n",
    "        for _ in range(num_samples):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, labels) + self.kl_divergence * complexity_cost_weight\n",
    "        return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb4cd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f33be18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1, complexity_cost_weight=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model.sample_elbo(data, target, criterion, num_samples, complexity_cost_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target) + model.kl_divergence\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "    total = len(test_loader.dataset)\n",
    "    error = (total - correct) / total\n",
    "\n",
    "    print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
    "    return total_loss / total, error\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, complexity_cost_weight=1, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples, complexity_cost_weight)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8a0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mnist(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd095371",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, generator=generator)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, generator=generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61306f93",
   "metadata": {},
   "source": [
    "# Grid search with wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4486190a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wandb-api-key')\n",
    "\n",
    "wandb.login(key=key)\n",
    "\n",
    "\n",
    "def train_wrapper():\n",
    "    with wandb.init(project=\"asi-paper\") as run:\n",
    "        model = train_mnist(\n",
    "            train_loader,\n",
    "            val_loader,\n",
    "            epochs=50,\n",
    "            lr=run.config.lr,\n",
    "            num_samples=run.config.sample_nbr,\n",
    "            pi=run.config.pi,\n",
    "            minus_log_sigma1=run.config.min_log_sigma1,\n",
    "            minus_log_sigma2=run.config.min_log_sigma2,\n",
    "            use_wandb=True\n",
    "        )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"method\": \"grid\",\n",
    "    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_loss\"},\n",
    "    'name': \"sweep-mnist\",\n",
    "    \"parameters\": {\n",
    "        \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
    "        \"sample_nbr\": {'values': [1, 2, 5, 10]},\n",
    "        \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "        \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
    "        \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
    "    },\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "wandb.agent(sweep_id, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b83bc6",
   "metadata": {},
   "source": [
    "# Manual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ca1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: 1142/10000 (11.42%)\n",
      "Epoch 1/10, Train Loss: 23315577.5524, Val Loss: 182165.4018, Val Error: 0.8858\n",
      "Correct: 1129/10000 (11.29%)\n",
      "Epoch 2/10, Train Loss: 22577394.2813, Val Loss: 174537.8510, Val Error: 0.8871\n",
      "Correct: 1129/10000 (11.29%)\n",
      "Epoch 3/10, Train Loss: 21613437.2072, Val Loss: 166938.8414, Val Error: 0.8871\n",
      "Correct: 1129/10000 (11.29%)\n",
      "Epoch 4/10, Train Loss: 20654729.0895, Val Loss: 159400.0020, Val Error: 0.8871\n",
      "Correct: 1129/10000 (11.29%)\n",
      "Epoch 5/10, Train Loss: 19707064.8440, Val Loss: 151981.1196, Val Error: 0.8871\n",
      "Correct: 1134/10000 (11.34%)\n",
      "Epoch 6/10, Train Loss: 18779091.5396, Val Loss: 144756.3308, Val Error: 0.8866\n",
      "Correct: 1131/10000 (11.31%)\n",
      "Epoch 7/10, Train Loss: 17880095.1662, Val Loss: 137756.2638, Val Error: 0.8869\n",
      "Correct: 1380/10000 (13.80%)\n",
      "Epoch 8/10, Train Loss: 16994142.1023, Val Loss: 130732.0625, Val Error: 0.8620\n",
      "Correct: 1017/10000 (10.17%)\n",
      "Epoch 9/10, Train Loss: 16139784.3018, Val Loss: 124497.7753, Val Error: 0.8983\n",
      "Correct: 1103/10000 (11.03%)\n",
      "Epoch 10/10, Train Loss: 15451323.4066, Val Loss: 119838.1370, Val Error: 0.8897\n"
     ]
    }
   ],
   "source": [
    "# model = train_mnist(train_loader, val_loader, epochs=10, lr=0.01, num_samples=5, pi=0.3, minus_log_sigma1=2, minus_log_sigma2=6)\n",
    "# torch.save(model.state_dict(), \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

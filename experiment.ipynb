{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torchvision import datasets\nfrom torchvision.transforms import v2 as transforms\nimport numpy as np\nimport wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:08.496227Z","iopub.execute_input":"2025-05-13T13:54:08.496914Z","iopub.status.idle":"2025-05-13T13:54:18.188437Z","shell.execute_reply.started":"2025-05-13T13:54:08.496880Z","shell.execute_reply":"2025-05-13T13:54:18.187899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainableNormalDistribution(nn.Module):\n    LOG_SQRT2PI = np.log(np.sqrt(2 * np.pi))\n\n    def __init__(self, mu, rho):\n        super().__init__()\n\n        self.mu = nn.Parameter(mu)\n        self.rho = nn.Parameter(rho)\n        self.register_buffer('eps', torch.Tensor(self.mu.shape))\n        self.sigma = None\n        self.w = None\n\n    def sample(self):\n        self.eps.data.normal_()\n        self.sigma = torch.log1p(torch.exp(self.rho))\n        self.w = self.mu + self.sigma * self.eps\n        return self.w\n\n    def log_posterior(self):\n        assert (self.w is not None), \"You can only have a log posterior for W if you've already sampled it\"\n\n        log_posteriors = -TrainableNormalDistribution.LOG_SQRT2PI - torch.log(self.sigma) - (((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)) - 0.5\n        return log_posteriors.sum()\n\n\nclass PriorWeightDistribution(nn.Module):\n    # Calculates a Scale Mixture Prior distribution for the prior part of the complexity cost on Bayes by Backprop paper\n    def __init__(self, pi, sigma1, sigma2):\n        super().__init__()\n\n        self.pi = pi\n        self.sigma1 = sigma1\n        self.sigma2 = sigma2\n        self.dist1 = torch.distributions.Normal(0, sigma1)\n        self.dist2 = torch.distributions.Normal(0, sigma2)\n\n    def log_prior(self, w):\n        prob_n1 = torch.exp(self.dist1.log_prob(w))\n        prob_n2 = torch.exp(self.dist2.log_prob(w))\n\n        # Prior of the mixture distribution, adding 1e-6 prevents numeric problems with log(p) for small p\n        prior_pdf = (self.pi * prob_n1 + (1 - self.pi) * prob_n2) + 1e-6\n\n        return (torch.log(prior_pdf) - 0.5).sum()\n\n\nclass BayesianLinear(nn.Module):\n    def __init__(self, in_features, out_features, bias=True, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0, prior_dist=None):\n        super().__init__()\n\n        # our main parameters\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n\n        # parameters for the scale mixture prior\n        self.prior_sigma_1 = prior_sigma_1\n        self.prior_sigma_2 = prior_sigma_2\n        self.prior_pi = prior_pi\n        self.prior_dist = prior_dist\n\n        # Variational weight parameters and sample\n        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_mu_init, 0.1))\n        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features).normal_(posterior_rho_init, 0.1))\n        self.weight_sampler = TrainableNormalDistribution(self.weight_mu, self.weight_rho)\n\n        # Variational bias parameters and sample\n        self.bias_mu = nn.Parameter(torch.Tensor(out_features).normal_(posterior_mu_init, 0.1))\n        self.bias_rho = nn.Parameter(torch.Tensor(out_features).normal_(posterior_rho_init, 0.1))\n        self.bias_sampler = TrainableNormalDistribution(self.bias_mu, self.bias_rho)\n\n        # Priors (as BBP paper)\n        self.weight_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n        self.bias_prior_dist = PriorWeightDistribution(self.prior_pi, self.prior_sigma_1, self.prior_sigma_2)\n        self.log_prior = 0\n        self.log_variational_posterior = 0\n\n    def forward(self, x):\n        # Sample the weights and forward it\n        w = self.weight_sampler.sample()\n\n        if self.bias:\n            b = self.bias_sampler.sample()\n            b_log_posterior = self.bias_sampler.log_posterior()\n            b_log_prior = self.bias_prior_dist.log_prior(b)\n        else:\n            b = torch.zeros((self.out_features), device=x.device)\n            b_log_posterior = 0\n            b_log_prior = 0\n\n        # Get the complexity cost\n        self.log_variational_posterior = self.weight_sampler.log_posterior() + b_log_posterior\n        self.log_prior = self.weight_prior_dist.log_prior(w) + b_log_prior\n\n        # print(x.shape, w.shape, b.shape)\n        return F.linear(x, w, b)\n\n    @property\n    def kl_divergence(self):\n        return self.log_variational_posterior - self.log_prior\n\n\ndef minibatch_weight(batch_idx: int, num_batches: int) -> float:\n    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:18.189724Z","iopub.execute_input":"2025-05-13T13:54:18.190106Z","iopub.status.idle":"2025-05-13T13:54:18.202374Z","shell.execute_reply.started":"2025-05-13T13:54:18.190088Z","shell.execute_reply":"2025-05-13T13:54:18.201575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MNISTModel(nn.Module):\n\n    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1, posterior_mu_init=0, posterior_rho_init=-7.0):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            BayesianLinear(\n                in_features, in_features,\n                prior_sigma_1=prior_sigma_1,\n                prior_sigma_2=prior_sigma_2,\n                prior_pi=prior_pi,\n                posterior_mu_init=posterior_mu_init,\n                posterior_rho_init=posterior_rho_init\n            ),\n            nn.ReLU(),\n            BayesianLinear(\n                in_features, in_features,\n                prior_sigma_1=prior_sigma_1,\n                prior_sigma_2=prior_sigma_2,\n                prior_pi=prior_pi,\n                posterior_mu_init=posterior_mu_init,\n                posterior_rho_init=posterior_rho_init\n            ),\n            nn.ReLU(),\n            BayesianLinear(\n                in_features, out_features,\n                prior_sigma_1=prior_sigma_1,\n                prior_sigma_2=prior_sigma_2,\n                prior_pi=prior_pi,\n                posterior_mu_init=posterior_mu_init,\n                posterior_rho_init=posterior_rho_init\n            ),\n            nn.Softmax(dim=1),\n        )\n\n    def forward(self, x):\n        x = self.layers(x)\n        # print(x)\n        return x\n\n    @property\n    def kl_divergence(self):\n        kl = 0\n        for module in self.modules():\n            kl += getattr(module, 'kl_divergence', 0) if module != self else 0\n        return kl\n\n    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n        loss = 0\n        for _ in range(num_samples):\n            outputs = self(inputs)\n            contr1 = criterion(outputs, labels)\n            contr2 = self.kl_divergence * complexity_cost_weight\n            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n            loss += contr1\n        return loss / num_samples","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:18.203202Z","iopub.execute_input":"2025-05-13T13:54:18.203504Z","iopub.status.idle":"2025-05-13T13:54:18.236030Z","shell.execute_reply.started":"2025-05-13T13:54:18.203479Z","shell.execute_reply":"2025-05-13T13:54:18.235554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:18.236718Z","iopub.execute_input":"2025-05-13T13:54:18.236903Z","iopub.status.idle":"2025-05-13T13:54:18.303530Z","shell.execute_reply.started":"2025-05-13T13:54:18.236889Z","shell.execute_reply":"2025-05-13T13:54:18.302959Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n    model.train()\n\n    total_loss = 0\n\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n\n        optimizer.zero_grad()\n\n        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n\n        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(train_loader)\n\n\ndef evaluate(model, test_loader, criterion):\n    model.eval()\n\n    total_loss = 0\n    correct = 0\n\n    with torch.no_grad():\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data, target = data.to(device), target.to(device)\n\n            output = model(data)\n\n            loss = criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(test_loader))\n            total_loss += loss.item()\n\n            preds = torch.argmax(output, 1)\n            correct += (preds == target).sum().item()\n\n            # print(f\"Predictions: {preds}, Targets: {target}\")\n\n    total = len(test_loader.dataset)\n    error = (total - correct) / total\n\n    print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n    return total_loss / total, error\n\n\ndef train(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n    for epoch in range(num_epochs):\n        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n        val_loss, val_error = evaluate(model, val_loader, criterion)\n\n        if use_wandb:\n            wandb.log({\n                \"epoch\": epoch,\n                \"train_loss\": train_loss,\n                \"val_loss\": val_loss,\n                \"val_error\": val_error\n            })\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:18.305217Z","iopub.execute_input":"2025-05-13T13:54:18.305444Z","iopub.status.idle":"2025-05-13T13:54:18.322476Z","shell.execute_reply.started":"2025-05-13T13:54:18.305420Z","shell.execute_reply":"2025-05-13T13:54:18.321946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_mnist(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n    sigma1 = np.exp(-minus_log_sigma1)\n    sigma2 = np.exp(-minus_log_sigma2)\n\n    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n    model.to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss(reduction='sum')\n\n    # if use_wandb:\n    #     wandb.init(project=\"asi-paper\", name=\"mnist\")\n\n    train(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:18.323142Z","iopub.execute_input":"2025-05-13T13:54:18.323373Z","iopub.status.idle":"2025-05-13T13:54:18.337588Z","shell.execute_reply.started":"2025-05-13T13:54:18.323353Z","shell.execute_reply":"2025-05-13T13:54:18.336887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size = 128\ntransform = transforms.Compose([\n    transforms.ToImage(),\n    transforms.ToDtype(torch.float32, scale=True),\n    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n])\n\n\nmnist_dataset = datasets.MNIST(\n    root=\"./mnist\",\n    download=True,\n    transform=transform,\n    train=True\n)\n\ngenerator = torch.Generator().manual_seed(42)\ntrain_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n\nkwargs = {\n    'batch_size': batch_size,\n    'num_workers': 4,\n    'generator': generator,\n}\n\ntrain_loader = torch.utils.data.DataLoader(\n    train_dataset,\n    shuffle=True,\n    **kwargs\n)\nval_loader = torch.utils.data.DataLoader(\n    val_dataset,\n    shuffle=False,\n    **kwargs\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:55:10.263590Z","iopub.execute_input":"2025-05-13T13:55:10.263892Z","iopub.status.idle":"2025-05-13T13:55:10.355141Z","shell.execute_reply.started":"2025-05-13T13:55:10.263865Z","shell.execute_reply":"2025-05-13T13:55:10.354536Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Grid search with wandb","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nkey = user_secrets.get_secret('wand-api-key-asi')\n\nwandb.login(key=key)\n\n\ndef train_wrapper():\n    with wandb.init(project=\"asi-paper\") as run:\n        model = train_mnist(\n            train_loader,\n            val_loader,\n            epochs=10,\n            lr=run.config.lr,\n            num_samples=run.config.sample_nbr,\n            pi=run.config.pi,\n            minus_log_sigma1=run.config.min_log_sigma1,\n            minus_log_sigma2=run.config.min_log_sigma2,\n            use_wandb=True\n        )\n\n    return model\n\n\nsweep_configuration = {\n    \"method\": \"grid\",\n    \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n    'name': \"sweep-mnist\",\n    \"parameters\": {\n        \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n        \"sample_nbr\": {'values': [1, 2, 5, 10]},\n        \"pi\": {'values': [0.25, 0.5, 0.75]},\n        \"min_log_sigma1\": {'values': [0, 1, 2]},\n        \"min_log_sigma2\": {'values': [6, 7, 8]},\n    },\n}\n\nsweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\nwandb.agent(sweep_id, function=train_wrapper)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:22.194421Z","iopub.execute_input":"2025-05-13T13:54:22.194662Z","iopub.status.idle":"2025-05-13T13:54:22.198249Z","shell.execute_reply.started":"2025-05-13T13:54:22.194636Z","shell.execute_reply":"2025-05-13T13:54:22.197481Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Manual training","metadata":{}},{"cell_type":"code","source":"# model = train_mnist(train_loader, val_loader, epochs=10, lr=0.01, num_samples=5, pi=0.3, minus_log_sigma1=2, minus_log_sigma2=6)\n# torch.save(model.state_dict(), \"mnist_model.pt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:55:14.261068Z","iopub.execute_input":"2025-05-13T13:55:14.261842Z","iopub.status.idle":"2025-05-13T13:55:48.075758Z","shell.execute_reply.started":"2025-05-13T13:55:14.261809Z","shell.execute_reply":"2025-05-13T13:55:48.074644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# model.load_state_dict(torch.load(\"mnist_model.pt\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-13T13:54:58.599751Z","iopub.status.idle":"2025-05-13T13:54:58.599980Z","shell.execute_reply.started":"2025-05-13T13:54:58.599861Z","shell.execute_reply":"2025-05-13T13:54:58.599871Z"}},"outputs":[],"execution_count":null}]}
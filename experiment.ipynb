{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66b7ef67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:02.639022Z",
     "iopub.status.busy": "2025-06-02T12:41:02.638757Z",
     "iopub.status.idle": "2025-06-02T12:41:16.589683Z",
     "shell.execute_reply": "2025-06-02T12:41:16.588831Z"
    },
    "papermill": {
     "duration": 13.957449,
     "end_time": "2025-06-02T12:41:16.591165",
     "exception": false,
     "start_time": "2025-06-02T12:41:02.633716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cded9eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.599712Z",
     "iopub.status.busy": "2025-06-02T12:41:16.599326Z",
     "iopub.status.idle": "2025-06-02T12:41:16.603077Z",
     "shell.execute_reply": "2025-06-02T12:41:16.602379Z"
    },
    "papermill": {
     "duration": 0.008978,
     "end_time": "2025-06-02T12:41:16.604140",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.595162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "628bb88b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.611635Z",
     "iopub.status.busy": "2025-06-02T12:41:16.611394Z",
     "iopub.status.idle": "2025-06-02T12:41:16.666663Z",
     "shell.execute_reply": "2025-06-02T12:41:16.666025Z"
    },
    "papermill": {
     "duration": 0.059944,
     "end_time": "2025-06-02T12:41:16.667670",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.607726",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "213e363d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.675342Z",
     "iopub.status.busy": "2025-06-02T12:41:16.675134Z",
     "iopub.status.idle": "2025-06-02T12:41:16.685825Z",
     "shell.execute_reply": "2025-06-02T12:41:16.685133Z"
    },
    "papermill": {
     "duration": 0.01575,
     "end_time": "2025-06-02T12:41:16.686849",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.671099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class GaussianPosterior(nn.Module):\n",
    "    LOG_SQRT_2PI = 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self):\n",
    "        assert self.w is not None\n",
    "        assert self.sigma is not None\n",
    "\n",
    "        log_posterior = -GaussianPosterior.LOG_SQRT_2PI - torch.log(self.sigma) - ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "\n",
    "        return log_posterior.sum()\n",
    "\n",
    "\n",
    "class ScaleMixturePrior(nn.Module):\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w):\n",
    "        likelihood1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_mixture = self.pi * likelihood1 + (1 - self.pi) * likelihood2\n",
    "        log_prob = torch.log(p_mixture).sum()\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class BayesianModule(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            prior_pi: float,\n",
    "            prior_sigma1: float,\n",
    "            prior_sigma2: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
    "        w_rho = torch.empty(out_features, in_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
    "\n",
    "        bias_mu = torch.empty(out_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
    "        bias_rho = torch.empty(out_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
    "\n",
    "        self.w_posterior = GaussianPosterior(w_mu, w_rho)\n",
    "        self.b_posterior = GaussianPosterior(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.b_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.b_posterior.sample()\n",
    "\n",
    "        log_prior = self.w_prior.log_prior(w) + self.b_prior.log_prior(b)\n",
    "        log_posterior = self.w_posterior.log_posterior() + self.b_posterior.log_posterior()\n",
    "\n",
    "        self.kl_divergence = log_posterior - log_prior\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "\n",
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 1 / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0ea1f8",
   "metadata": {
    "papermill": {
     "duration": 0.003254,
     "end_time": "2025-06-02T12:41:16.693648",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.690394",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef12df06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.700964Z",
     "iopub.status.busy": "2025-06-02T12:41:16.700744Z",
     "iopub.status.idle": "2025-06-02T12:41:16.706429Z",
     "shell.execute_reply": "2025-06-02T12:41:16.705933Z"
    },
    "papermill": {
     "duration": 0.010509,
     "end_time": "2025-06-02T12:41:16.707390",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.696881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                1200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                1200,\n",
    "                1200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                1200,\n",
    "                out_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2,\n",
    "            ),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
    "        loss = 0\n",
    "        for _ in range(num_samples):\n",
    "            outputs = self(inputs)\n",
    "            contr1 = criterion(outputs, labels)\n",
    "            contr2 = self.kl_divergence * complexity_cost_weight\n",
    "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
    "            loss += contr1 + contr2\n",
    "        return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c05598f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.715155Z",
     "iopub.status.busy": "2025-06-02T12:41:16.714631Z",
     "iopub.status.idle": "2025-06-02T12:41:16.723779Z",
     "shell.execute_reply": "2025-06-02T12:41:16.723088Z"
    },
    "papermill": {
     "duration": 0.014146,
     "end_time": "2025-06-02T12:41:16.724826",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.710680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
    "\n",
    "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "            loss = (\n",
    "                criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(val_loader))\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total = len(val_loader.dataset)\n",
    "    return total_loss / total, (total - correct) / total\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "    total = len(test_loader.dataset)\n",
    "    error = (total - correct) / total\n",
    "\n",
    "    # print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
    "    return error\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "        test_error = test(model, test_loader)\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error,\n",
    "                \"test_error\": test_error\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.2%}, Test Error: {test_error:.2%}, Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b26cd9f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.732036Z",
     "iopub.status.busy": "2025-06-02T12:41:16.731839Z",
     "iopub.status.idle": "2025-06-02T12:41:16.736570Z",
     "shell.execute_reply": "2025-06-02T12:41:16.735898Z"
    },
    "papermill": {
     "duration": 0.009519,
     "end_time": "2025-06-02T12:41:16.737623",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.728104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_mnist(train_loader, val_loader, test_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    if use_wandb:\n",
    "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    train(model, train_loader, val_loader, test_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    if use_wandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb16fc9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:16.744959Z",
     "iopub.status.busy": "2025-06-02T12:41:16.744756Z",
     "iopub.status.idle": "2025-06-02T12:41:26.893009Z",
     "shell.execute_reply": "2025-06-02T12:41:26.891991Z"
    },
    "papermill": {
     "duration": 10.153346,
     "end_time": "2025-06-02T12:41:26.894278",
     "exception": false,
     "start_time": "2025-06-02T12:41:16.740932",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:01<00:00, 6.07MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 160kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.52MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 7.02MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "# transformed_data = transform(mnist_dataset.data).to(device)\n",
    "# y = mnist_dataset.targets.to(device)\n",
    "# mnist_dataset = torch.utils.data.TensorDataset(transformed_data, y)\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
    "\n",
    "kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 1,\n",
    "    'generator': generator,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")\n",
    "full_train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867111d",
   "metadata": {
    "papermill": {
     "duration": 0.004954,
     "end_time": "2025-06-02T12:41:26.905785",
     "exception": false,
     "start_time": "2025-06-02T12:41:26.900831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grid search with wandb\n",
    "Uncomment the code below to run a grid search and log the results to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe701af7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:26.916051Z",
     "iopub.status.busy": "2025-06-02T12:41:26.915464Z",
     "iopub.status.idle": "2025-06-02T12:41:26.919147Z",
     "shell.execute_reply": "2025-06-02T12:41:26.918598Z"
    },
    "papermill": {
     "duration": 0.009892,
     "end_time": "2025-06-02T12:41:26.920184",
     "exception": false,
     "start_time": "2025-06-02T12:41:26.910292",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret('wand-api-key-asi')\n",
    "# sweep_continue = user_secrets.get_secret('asi-mnist-sweep-id')\n",
    "\n",
    "# wandb.login(key=key)\n",
    "\n",
    "\n",
    "# def train_wrapper():\n",
    "#     with wandb.init(project=\"asi-paper\") as run:\n",
    "#         model = train_mnist(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             test_loader,\n",
    "#             epochs=10,\n",
    "#             lr=run.config.lr,\n",
    "#             num_samples=run.config.sample_nbr,\n",
    "#             pi=run.config.pi,\n",
    "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
    "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
    "#             use_wandb=True\n",
    "#         )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # sweep_configuration = {\n",
    "# #     \"method\": \"grid\",\n",
    "# #     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n",
    "# #     'name': \"sweep-mnist\",\n",
    "# #     \"parameters\": {\n",
    "# #         \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
    "# #         \"sample_nbr\": {'values': [1, 2, 3, 5]},\n",
    "# #         \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "# #         \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
    "# #         \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
    "# #     },\n",
    "# # }\n",
    "\n",
    "# # sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "# # print(f\"Sweep ID: {sweep_id}\")\n",
    "# wandb.agent(sweep_continue, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f642361",
   "metadata": {
    "papermill": {
     "duration": 0.004234,
     "end_time": "2025-06-02T12:41:26.928940",
     "exception": false,
     "start_time": "2025-06-02T12:41:26.924706",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual training\n",
    "Uncomment the code below to train the model with specified hyperparameters and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8dfb6c70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:26.938936Z",
     "iopub.status.busy": "2025-06-02T12:41:26.938734Z",
     "iopub.status.idle": "2025-06-02T12:41:26.944780Z",
     "shell.execute_reply": "2025-06-02T12:41:26.944066Z"
    },
    "papermill": {
     "duration": 0.012462,
     "end_time": "2025-06-02T12:41:26.945761",
     "exception": false,
     "start_time": "2025-06-02T12:41:26.933299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_final(train_loader, val_loader, epochs=50, lr=1e-3, num_samples=3, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6, use_wandb=True):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    if use_wandb:\n",
    "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error,\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.2%}, Time: {elapsed:.2f}s\")\n",
    "\n",
    "    if use_wandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9897440b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T12:41:26.955257Z",
     "iopub.status.busy": "2025-06-02T12:41:26.955053Z",
     "iopub.status.idle": "2025-06-02T13:23:19.652671Z",
     "shell.execute_reply": "2025-06-02T13:23:19.651870Z"
    },
    "papermill": {
     "duration": 2512.703791,
     "end_time": "2025-06-02T13:23:19.654004",
     "exception": false,
     "start_time": "2025-06-02T12:41:26.950213",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-ghia\u001b[0m (\u001b[33mmatteo-ghia-2001\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250602_124128-owqj188y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmnist\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper/runs/owqj188y\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 17289.6424, Val Loss: 671.6394, Val Error: 14.75%, Time: 55.30s\n",
      "Epoch 2/50, Train Loss: 13393.2142, Val Loss: 571.4220, Val Error: 9.82%, Time: 49.75s\n",
      "Epoch 3/50, Train Loss: 11314.4361, Val Loss: 474.1558, Val Error: 9.59%, Time: 50.68s\n",
      "Epoch 4/50, Train Loss: 9344.5042, Val Loss: 390.5471, Val Error: 9.20%, Time: 49.50s\n",
      "Epoch 5/50, Train Loss: 7852.6975, Val Loss: 334.8559, Val Error: 9.20%, Time: 51.46s\n",
      "Epoch 6/50, Train Loss: 6930.7381, Val Loss: 302.4638, Val Error: 8.96%, Time: 53.31s\n",
      "Epoch 7/50, Train Loss: 6401.3755, Val Loss: 283.7884, Val Error: 9.00%, Time: 51.52s\n",
      "Epoch 8/50, Train Loss: 6087.2564, Val Loss: 272.2376, Val Error: 8.97%, Time: 51.99s\n",
      "Epoch 9/50, Train Loss: 5883.5541, Val Loss: 264.3164, Val Error: 9.21%, Time: 51.09s\n",
      "Epoch 10/50, Train Loss: 5735.9754, Val Loss: 258.2136, Val Error: 9.06%, Time: 50.98s\n",
      "Epoch 11/50, Train Loss: 5618.4322, Val Loss: 253.1820, Val Error: 9.28%, Time: 50.42s\n",
      "Epoch 12/50, Train Loss: 5516.5795, Val Loss: 248.6656, Val Error: 9.71%, Time: 52.42s\n",
      "Epoch 13/50, Train Loss: 5423.8122, Val Loss: 244.4134, Val Error: 8.67%, Time: 49.91s\n",
      "Epoch 14/50, Train Loss: 5335.5549, Val Loss: 240.3897, Val Error: 9.76%, Time: 50.58s\n",
      "Epoch 15/50, Train Loss: 5250.2085, Val Loss: 236.3999, Val Error: 8.70%, Time: 50.58s\n",
      "Epoch 16/50, Train Loss: 5166.3922, Val Loss: 232.4802, Val Error: 9.03%, Time: 49.95s\n",
      "Epoch 17/50, Train Loss: 5083.9188, Val Loss: 228.6887, Val Error: 9.36%, Time: 50.93s\n",
      "Epoch 18/50, Train Loss: 5002.8731, Val Loss: 224.8945, Val Error: 9.68%, Time: 50.07s\n",
      "Epoch 19/50, Train Loss: 4923.3142, Val Loss: 221.2088, Val Error: 9.29%, Time: 50.77s\n",
      "Epoch 20/50, Train Loss: 4846.4165, Val Loss: 217.6447, Val Error: 9.80%, Time: 48.03s\n",
      "Epoch 21/50, Train Loss: 4772.2824, Val Loss: 214.2287, Val Error: 9.78%, Time: 49.92s\n",
      "Epoch 22/50, Train Loss: 4701.6169, Val Loss: 210.9871, Val Error: 9.85%, Time: 46.79s\n",
      "Epoch 23/50, Train Loss: 4634.8200, Val Loss: 207.9573, Val Error: 10.42%, Time: 48.44s\n",
      "Epoch 24/50, Train Loss: 4572.0597, Val Loss: 205.1168, Val Error: 10.26%, Time: 49.22s\n",
      "Epoch 25/50, Train Loss: 4514.1974, Val Loss: 202.5014, Val Error: 11.06%, Time: 49.18s\n",
      "Epoch 26/50, Train Loss: 4460.5130, Val Loss: 200.1068, Val Error: 10.73%, Time: 47.63s\n",
      "Epoch 27/50, Train Loss: 4411.6153, Val Loss: 197.8616, Val Error: 11.00%, Time: 48.61s\n",
      "Epoch 28/50, Train Loss: 4366.4361, Val Loss: 195.8728, Val Error: 11.55%, Time: 47.48s\n",
      "Epoch 29/50, Train Loss: 4325.5807, Val Loss: 194.0351, Val Error: 11.34%, Time: 48.49s\n",
      "Epoch 30/50, Train Loss: 4288.4549, Val Loss: 192.3943, Val Error: 12.48%, Time: 53.33s\n",
      "Epoch 31/50, Train Loss: 4254.6077, Val Loss: 190.8615, Val Error: 12.00%, Time: 50.45s\n",
      "Epoch 32/50, Train Loss: 4224.2103, Val Loss: 189.4747, Val Error: 12.40%, Time: 49.58s\n",
      "Epoch 33/50, Train Loss: 4196.4692, Val Loss: 188.2637, Val Error: 12.22%, Time: 49.03s\n",
      "Epoch 34/50, Train Loss: 4171.5396, Val Loss: 187.1433, Val Error: 11.83%, Time: 48.69s\n",
      "Epoch 35/50, Train Loss: 4148.9467, Val Loss: 186.1489, Val Error: 12.07%, Time: 49.81s\n",
      "Epoch 36/50, Train Loss: 4128.7433, Val Loss: 185.2363, Val Error: 11.96%, Time: 48.22s\n",
      "Epoch 37/50, Train Loss: 4110.5247, Val Loss: 184.4009, Val Error: 11.89%, Time: 50.84s\n",
      "Epoch 38/50, Train Loss: 4093.9648, Val Loss: 183.6922, Val Error: 11.46%, Time: 50.66s\n",
      "Epoch 39/50, Train Loss: 4079.0485, Val Loss: 183.0138, Val Error: 12.11%, Time: 50.17s\n",
      "Epoch 40/50, Train Loss: 4065.6682, Val Loss: 182.4516, Val Error: 11.79%, Time: 50.57s\n",
      "Epoch 41/50, Train Loss: 4053.6054, Val Loss: 181.9097, Val Error: 11.86%, Time: 48.69s\n",
      "Epoch 42/50, Train Loss: 4042.5804, Val Loss: 181.4092, Val Error: 11.83%, Time: 48.90s\n",
      "Epoch 43/50, Train Loss: 4032.5524, Val Loss: 180.9976, Val Error: 11.86%, Time: 51.32s\n",
      "Epoch 44/50, Train Loss: 4023.5624, Val Loss: 180.5664, Val Error: 12.20%, Time: 49.65s\n",
      "Epoch 45/50, Train Loss: 4015.3438, Val Loss: 180.2412, Val Error: 12.74%, Time: 49.22s\n",
      "Epoch 46/50, Train Loss: 4008.0191, Val Loss: 179.8980, Val Error: 11.94%, Time: 50.76s\n",
      "Epoch 47/50, Train Loss: 4001.3692, Val Loss: 179.5654, Val Error: 11.93%, Time: 52.30s\n",
      "Epoch 48/50, Train Loss: 3995.1130, Val Loss: 179.2922, Val Error: 11.98%, Time: 49.55s\n",
      "Epoch 49/50, Train Loss: 3989.5330, Val Loss: 179.0677, Val Error: 11.49%, Time: 50.51s\n",
      "Epoch 50/50, Train Loss: 3984.2790, Val Loss: 178.8306, Val Error: 11.59%, Time: 50.64s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading output.log; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▅▄▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_error █▂▂▂▂▁▁▂▁▂▁▂▁▁▂▂▂▂▂▃▄▃▄▄▄▅▅▅▅▅▅▄▅▅▅▅▅▅▅▄\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 3984.27897\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_error 0.1159\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 178.83055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmnist\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper/runs/owqj188y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250602_124128-owqj188y/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wand-api-key-asi')\n",
    "\n",
    "wandb.login(key=key)\n",
    "\n",
    "# model = train_mnist(train_loader, val_loader, test_loader, epochs=50, lr=1e-3, num_samples=3, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6, use_wandb=True)\n",
    "model = train_final(full_train_loader, test_loader)\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de293e12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.670043Z",
     "iopub.status.busy": "2025-06-02T13:23:19.669817Z",
     "iopub.status.idle": "2025-06-02T13:23:19.672990Z",
     "shell.execute_reply": "2025-06-02T13:23:19.672488Z"
    },
    "papermill": {
     "duration": 0.012381,
     "end_time": "2025-06-02T13:23:19.674013",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.661632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = MNISTModel(prior_sigma_1=np.exp(-1), prior_sigma_2=np.exp(-7), prior_pi=0.75)\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98a44f",
   "metadata": {
    "papermill": {
     "duration": 0.006969,
     "end_time": "2025-06-02T13:23:19.688061",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.681092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regression curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa5b5d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.702812Z",
     "iopub.status.busy": "2025-06-02T13:23:19.702608Z",
     "iopub.status.idle": "2025-06-02T13:23:19.708778Z",
     "shell.execute_reply": "2025-06-02T13:23:19.708253Z"
    },
    "papermill": {
     "duration": 0.014887,
     "end_time": "2025-06-02T13:23:19.709839",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.694952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_samples(num_samples):\n",
    "    eps = np.random.normal(0, 0.02, num_samples)\n",
    "    x = np.linspace(0, 0.5, num_samples)\n",
    "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def save_samples(x, y, filename):\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def load_samples(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_samples(x, y):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, y, 'kx', label='Generated Samples')\n",
    "    plt.title('Generated Samples')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# x, y = generate_samples(1000)\n",
    "# save_samples(x, y, 'regression_samples.csv')\n",
    "\n",
    "\n",
    "# x, y = load_samples('regression_samples.csv')\n",
    "# plot_samples(x, y)\n",
    "\n",
    "# X_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1).to(device)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_tensor[:800], y_tensor[:800])\n",
    "# val_dataset = torch.utils.data.TensorDataset(X_tensor[800:], y_tensor[800:])\n",
    "\n",
    "\n",
    "# kwargs = {\n",
    "#     'batch_size': batch_size,\n",
    "#     'generator': generator,\n",
    "# }\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     shuffle=True,\n",
    "#     **kwargs\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     shuffle=False,\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "396dae69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.724652Z",
     "iopub.status.busy": "2025-06-02T13:23:19.724426Z",
     "iopub.status.idle": "2025-06-02T13:23:19.728981Z",
     "shell.execute_reply": "2025-06-02T13:23:19.728487Z"
    },
    "papermill": {
     "duration": 0.013088,
     "end_time": "2025-06-02T13:23:19.729928",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.716840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, in_features=1, out_features=1, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                200,\n",
    "                200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                200,\n",
    "                out_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "463a2c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.744362Z",
     "iopub.status.busy": "2025-06-02T13:23:19.744174Z",
     "iopub.status.idle": "2025-06-02T13:23:19.750429Z",
     "shell.execute_reply": "2025-06-02T13:23:19.749940Z"
    },
    "papermill": {
     "duration": 0.014755,
     "end_time": "2025-06-02T13:23:19.751454",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.736699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_regression(regressor, X, y, samples=100, std_multiplier=2):\n",
    "    preds = [regressor(X) for _ in range(samples)]\n",
    "    preds = torch.stack(preds)\n",
    "    means = preds.mean(axis=0)\n",
    "    stds = preds.std(axis=0)\n",
    "    ci_upper = means + (std_multiplier * stds)\n",
    "    ci_lower = means - (std_multiplier * stds)\n",
    "    ci_acc = (ci_lower <= y) * (ci_upper >= y)\n",
    "    ci_acc = ci_acc.float().mean()\n",
    "    return ci_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()\n",
    "\n",
    "\n",
    "def train_regression(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        ci_acc, ci_upper, ci_lower = evaluate_regression(model, val_loader.dataset.tensors[0], val_loader.dataset.tensors[1])\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"ci_acc\": ci_acc,\n",
    "                \"ci_upper\": ci_upper,\n",
    "                \"ci_lower\": ci_lower,\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, CI acc: {ci_acc}, CI upper acc: {ci_upper}, CI lower acc: {ci_lower} Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c5d7c590",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.766895Z",
     "iopub.status.busy": "2025-06-02T13:23:19.766304Z",
     "iopub.status.idle": "2025-06-02T13:23:19.770938Z",
     "shell.execute_reply": "2025-06-02T13:23:19.770442Z"
    },
    "papermill": {
     "duration": 0.013433,
     "end_time": "2025-06-02T13:23:19.771953",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.758520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_regression_model(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = RegressionModel(1, 1, prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     run = wandb.init(project=\"asi-paper\", name=\"regression\")\n",
    "\n",
    "    train_regression(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "252ac8bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.786888Z",
     "iopub.status.busy": "2025-06-02T13:23:19.786672Z",
     "iopub.status.idle": "2025-06-02T13:23:19.789933Z",
     "shell.execute_reply": "2025-06-02T13:23:19.789429Z"
    },
    "papermill": {
     "duration": 0.011993,
     "end_time": "2025-06-02T13:23:19.790933",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.778940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret('wand-api-key-asi')\n",
    "\n",
    "# wandb.login(key=key)\n",
    "\n",
    "\n",
    "# def train_wrapper():\n",
    "#     with wandb.init(project=\"asi-paper\") as run:\n",
    "#         model = train_regression_model(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             epochs=15,\n",
    "#             lr=run.config.lr,\n",
    "#             num_samples=run.config.sample_nbr,\n",
    "#             pi=run.config.pi,\n",
    "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
    "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
    "#             use_wandb=True\n",
    "#         )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# sweep_configuration = {\n",
    "#     \"method\": \"bayes\",\n",
    "#     \"metric\": {\"goal\": \"maximize\", \"name\": \"ci_acc\"},\n",
    "#     'name': \"sweep-regression\",\n",
    "#     \"parameters\": {\n",
    "#         \"lr\": {'min': 1e-5, 'max': 1e-2},\n",
    "#         \"sample_nbr\": {'min': 1, 'max': 10},\n",
    "#         \"pi\": {'min': 0.25, 'max': 0.75},\n",
    "#         \"min_log_sigma1\": {'min': 0, 'max': 2},\n",
    "#         \"min_log_sigma2\": {'min': 6, 'max': 8},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "# wandb.agent(sweep_id, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9215a018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.805581Z",
     "iopub.status.busy": "2025-06-02T13:23:19.805351Z",
     "iopub.status.idle": "2025-06-02T13:23:19.808494Z",
     "shell.execute_reply": "2025-06-02T13:23:19.807980Z"
    },
    "papermill": {
     "duration": 0.011756,
     "end_time": "2025-06-02T13:23:19.809615",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.797859",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = train_regression_model(train_loader, val_loader, epochs=10, lr=1e-3, num_samples=1, pi=0.5, minus_log_sigma1=0.5, minus_log_sigma2=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2353e831",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-02T13:23:19.824658Z",
     "iopub.status.busy": "2025-06-02T13:23:19.824388Z",
     "iopub.status.idle": "2025-06-02T13:23:19.827262Z",
     "shell.execute_reply": "2025-06-02T13:23:19.826799Z"
    },
    "papermill": {
     "duration": 0.011595,
     "end_time": "2025-06-02T13:23:19.828172",
     "exception": false,
     "start_time": "2025-06-02T13:23:19.816577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# predicted = model(X_tensor).cpu().detach().numpy()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(x, y, 'kx', label='Generated Samples')\n",
    "# plt.plot(x, predicted, 'r-', label='Predicted Mean')\n",
    "# # plt.fill_between(x, predicted - 2 * np.std(predicted), predicted + 2 * np.std(predicted), color='r', alpha=0.2, label='Uncertainty')\n",
    "# plt.title('Regression with Uncertainty')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2544.07044,
   "end_time": "2025-06-02T13:23:22.554222",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-02T12:40:58.483782",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

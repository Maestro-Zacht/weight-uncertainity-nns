{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4750d8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:11:51.910924Z",
     "iopub.status.busy": "2025-05-29T13:11:51.910509Z",
     "iopub.status.idle": "2025-05-29T13:12:07.290861Z",
     "shell.execute_reply": "2025-05-29T13:12:07.290046Z"
    },
    "papermill": {
     "duration": 15.389681,
     "end_time": "2025-05-29T13:12:07.292293",
     "exception": false,
     "start_time": "2025-05-29T13:11:51.902612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3b06816",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.300869Z",
     "iopub.status.busy": "2025-05-29T13:12:07.300495Z",
     "iopub.status.idle": "2025-05-29T13:12:07.303924Z",
     "shell.execute_reply": "2025-05-29T13:12:07.303450Z"
    },
    "papermill": {
     "duration": 0.008671,
     "end_time": "2025-05-29T13:12:07.304903",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.296232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0ec2b85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.312134Z",
     "iopub.status.busy": "2025-05-29T13:12:07.311929Z",
     "iopub.status.idle": "2025-05-29T13:12:07.372271Z",
     "shell.execute_reply": "2025-05-29T13:12:07.371563Z"
    },
    "papermill": {
     "duration": 0.065306,
     "end_time": "2025-05-29T13:12:07.373474",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.308168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05c36b17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.381599Z",
     "iopub.status.busy": "2025-05-29T13:12:07.381073Z",
     "iopub.status.idle": "2025-05-29T13:12:07.392207Z",
     "shell.execute_reply": "2025-05-29T13:12:07.391726Z"
    },
    "papermill": {
     "duration": 0.016134,
     "end_time": "2025-05-29T13:12:07.393234",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.377100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class GaussianPosterior(nn.Module):\n",
    "    LOG_SQRT_2PI = 0.5 * np.log(2 * np.pi)\n",
    "\n",
    "    def __init__(self, mu, rho):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self):\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log1p(torch.exp(self.rho))\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self):\n",
    "        assert self.w is not None\n",
    "        assert self.sigma is not None\n",
    "\n",
    "        log_posterior = -GaussianPosterior.LOG_SQRT_2PI - torch.log(self.sigma) - ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "\n",
    "        return log_posterior.sum()\n",
    "\n",
    "\n",
    "class ScaleMixturePrior(nn.Module):\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w):\n",
    "        likelihood1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_mixture = self.pi * likelihood1 + (1 - self.pi) * likelihood2\n",
    "        log_prob = torch.log(p_mixture).sum()\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class BayesianModule(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            prior_pi: float,\n",
    "            prior_sigma1: float,\n",
    "            prior_sigma2: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
    "        w_rho = torch.empty(out_features, in_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
    "\n",
    "        bias_mu = torch.empty(out_features).normal_(0.0, 0.01 * (np.log(in_features) + np.log(out_features)), generator=generator)\n",
    "        bias_rho = torch.empty(out_features).normal_(-4.5, 0.001 * (np.log(in_features) + np.log(out_features)))\n",
    "\n",
    "        self.w_posterior = GaussianPosterior(w_mu, w_rho)\n",
    "        self.b_posterior = GaussianPosterior(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.b_prior = ScaleMixturePrior(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.b_posterior.sample()\n",
    "\n",
    "        log_prior = self.w_prior.log_prior(w) + self.b_prior.log_prior(b)\n",
    "        log_posterior = self.w_posterior.log_posterior() + self.b_posterior.log_posterior()\n",
    "\n",
    "        self.kl_divergence = log_posterior - log_prior\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "\n",
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 1 / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00c8867",
   "metadata": {
    "papermill": {
     "duration": 0.003068,
     "end_time": "2025-05-29T13:12:07.399669",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.396601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73c29cf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.406822Z",
     "iopub.status.busy": "2025-05-29T13:12:07.406635Z",
     "iopub.status.idle": "2025-05-29T13:12:07.412548Z",
     "shell.execute_reply": "2025-05-29T13:12:07.412010Z"
    },
    "papermill": {
     "duration": 0.010706,
     "end_time": "2025-05-29T13:12:07.413550",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.402844",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                1200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                1200,\n",
    "                1200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                1200,\n",
    "                out_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2,\n",
    "            ),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
    "        loss = 0\n",
    "        for _ in range(num_samples):\n",
    "            outputs = self(inputs)\n",
    "            contr1 = criterion(outputs, labels)\n",
    "            contr2 = self.kl_divergence * complexity_cost_weight\n",
    "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
    "            loss += contr1 + contr2\n",
    "        return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb6dddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.421187Z",
     "iopub.status.busy": "2025-05-29T13:12:07.420991Z",
     "iopub.status.idle": "2025-05-29T13:12:07.429595Z",
     "shell.execute_reply": "2025-05-29T13:12:07.429064Z"
    },
    "papermill": {
     "duration": 0.01352,
     "end_time": "2025-05-29T13:12:07.430688",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.417168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
    "\n",
    "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "            loss = (\n",
    "                criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(val_loader))\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    total = len(val_loader.dataset)\n",
    "    return total_loss / total, (total - correct) / total\n",
    "\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "    total = len(test_loader.dataset)\n",
    "    error = (total - correct) / total\n",
    "\n",
    "    # print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
    "    return error\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, test_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "        test_error = test(model, test_loader)\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error,\n",
    "                \"test_error\": test_error\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.2%}, Test Error: {test_error:.2%}, Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3303b90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.437930Z",
     "iopub.status.busy": "2025-05-29T13:12:07.437698Z",
     "iopub.status.idle": "2025-05-29T13:12:07.442245Z",
     "shell.execute_reply": "2025-05-29T13:12:07.441735Z"
    },
    "papermill": {
     "duration": 0.009337,
     "end_time": "2025-05-29T13:12:07.443246",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.433909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_mnist(train_loader, val_loader, test_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    if use_wandb:\n",
    "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    train(model, train_loader, val_loader, test_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    if use_wandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05122849",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:07.450461Z",
     "iopub.status.busy": "2025-05-29T13:12:07.450194Z",
     "iopub.status.idle": "2025-05-29T13:12:12.698592Z",
     "shell.execute_reply": "2025-05-29T13:12:12.697739Z"
    },
    "papermill": {
     "duration": 5.253382,
     "end_time": "2025-05-29T13:12:12.699851",
     "exception": false,
     "start_time": "2025-05-29T13:12:07.446469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.8MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 338kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.20MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.54MB/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Lambda(lambda x: x.view(28 * 28) / 126.0),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "# transformed_data = transform(mnist_dataset.data).to(device)\n",
    "# y = mnist_dataset.targets.to(device)\n",
    "# mnist_dataset = torch.utils.data.TensorDataset(transformed_data, y)\n",
    "\n",
    "test_set = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=False\n",
    ")\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
    "\n",
    "kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 1,\n",
    "    'generator': generator,\n",
    "    'pin_memory': True,\n",
    "}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")\n",
    "full_train_loader = torch.utils.data.DataLoader(\n",
    "    mnist_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19a33de",
   "metadata": {
    "papermill": {
     "duration": 0.004305,
     "end_time": "2025-05-29T13:12:12.709687",
     "exception": false,
     "start_time": "2025-05-29T13:12:12.705382",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Grid search with wandb\n",
    "Uncomment the code below to run a grid search and log the results to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a83e012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:12.719326Z",
     "iopub.status.busy": "2025-05-29T13:12:12.718542Z",
     "iopub.status.idle": "2025-05-29T13:12:12.722551Z",
     "shell.execute_reply": "2025-05-29T13:12:12.721871Z"
    },
    "papermill": {
     "duration": 0.010034,
     "end_time": "2025-05-29T13:12:12.723636",
     "exception": false,
     "start_time": "2025-05-29T13:12:12.713602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret('wand-api-key-asi')\n",
    "# sweep_continue = user_secrets.get_secret('asi-mnist-sweep-id')\n",
    "\n",
    "# wandb.login(key=key)\n",
    "\n",
    "\n",
    "# def train_wrapper():\n",
    "#     with wandb.init(project=\"asi-paper\") as run:\n",
    "#         model = train_mnist(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             test_loader,\n",
    "#             epochs=10,\n",
    "#             lr=run.config.lr,\n",
    "#             num_samples=run.config.sample_nbr,\n",
    "#             pi=run.config.pi,\n",
    "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
    "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
    "#             use_wandb=True\n",
    "#         )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# # sweep_configuration = {\n",
    "# #     \"method\": \"grid\",\n",
    "# #     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n",
    "# #     'name': \"sweep-mnist\",\n",
    "# #     \"parameters\": {\n",
    "# #         \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
    "# #         \"sample_nbr\": {'values': [1, 2, 3, 5]},\n",
    "# #         \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "# #         \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
    "# #         \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
    "# #     },\n",
    "# # }\n",
    "\n",
    "# # sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "# # print(f\"Sweep ID: {sweep_id}\")\n",
    "# wandb.agent(sweep_continue, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9c1c8",
   "metadata": {
    "papermill": {
     "duration": 0.003841,
     "end_time": "2025-05-29T13:12:12.731534",
     "exception": false,
     "start_time": "2025-05-29T13:12:12.727693",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Manual training\n",
    "Uncomment the code below to train the model with specified hyperparameters and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209ad213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_final(train_loader, val_loader, epochs=50, lr=1e-3, num_samples=3, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6, use_wandb=True):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    if use_wandb:\n",
    "        run = wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error,\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.2%}, Time: {elapsed:.2f}s\")\n",
    "\n",
    "    if use_wandb:\n",
    "        run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb9cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:12:12.740537Z",
     "iopub.status.busy": "2025-05-29T13:12:12.740110Z",
     "iopub.status.idle": "2025-05-29T13:54:02.713272Z",
     "shell.execute_reply": "2025-05-29T13:54:02.712446Z"
    },
    "papermill": {
     "duration": 2509.979009,
     "end_time": "2025-05-29T13:54:02.714691",
     "exception": false,
     "start_time": "2025-05-29T13:12:12.735682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmatteo-ghia\u001b[0m (\u001b[33mmatteo-ghia-2001\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250529_131214-so7m9drh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmnist\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper/runs/so7m9drh\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Train Loss: 21300.0366, Val Loss: 694.7748, Val Error: 24.80%, Test Error: 24.68%, Time: 50.41s\n",
      "Epoch 2/50, Train Loss: 16688.8517, Val Loss: 602.8719, Val Error: 18.44%, Test Error: 18.78%, Time: 50.07s\n",
      "Epoch 3/50, Train Loss: 14554.4962, Val Loss: 521.5768, Val Error: 17.44%, Test Error: 17.53%, Time: 50.93s\n",
      "Epoch 4/50, Train Loss: 12489.6414, Val Loss: 443.1921, Val Error: 17.68%, Test Error: 17.72%, Time: 47.93s\n",
      "Epoch 5/50, Train Loss: 10653.0316, Val Loss: 379.8517, Val Error: 17.42%, Test Error: 17.49%, Time: 50.28s\n",
      "Epoch 6/50, Train Loss: 9287.0125, Val Loss: 336.2009, Val Error: 17.42%, Test Error: 17.47%, Time: 50.17s\n",
      "Epoch 7/50, Train Loss: 8388.4242, Val Loss: 308.5707, Val Error: 17.52%, Test Error: 17.58%, Time: 50.13s\n",
      "Epoch 8/50, Train Loss: 7819.9031, Val Loss: 291.0143, Val Error: 13.54%, Test Error: 13.50%, Time: 49.82s\n",
      "Epoch 9/50, Train Loss: 7450.0373, Val Loss: 279.3426, Val Error: 12.10%, Test Error: 11.37%, Time: 50.82s\n",
      "Epoch 10/50, Train Loss: 7198.3179, Val Loss: 271.0992, Val Error: 11.50%, Test Error: 10.83%, Time: 51.75s\n",
      "Epoch 11/50, Train Loss: 7014.2849, Val Loss: 264.8280, Val Error: 11.59%, Test Error: 10.80%, Time: 49.08s\n",
      "Epoch 12/50, Train Loss: 6869.2043, Val Loss: 259.7378, Val Error: 12.41%, Test Error: 11.38%, Time: 49.37s\n",
      "Epoch 13/50, Train Loss: 6747.4708, Val Loss: 255.2819, Val Error: 10.95%, Test Error: 10.14%, Time: 51.45s\n",
      "Epoch 14/50, Train Loss: 6639.8362, Val Loss: 251.2969, Val Error: 10.98%, Test Error: 10.43%, Time: 49.93s\n",
      "Epoch 15/50, Train Loss: 6541.0978, Val Loss: 247.5514, Val Error: 10.88%, Test Error: 10.43%, Time: 49.79s\n",
      "Epoch 16/50, Train Loss: 6447.7110, Val Loss: 244.0034, Val Error: 11.27%, Test Error: 10.05%, Time: 49.93s\n",
      "Epoch 17/50, Train Loss: 6357.6532, Val Loss: 240.5079, Val Error: 10.48%, Test Error: 9.75%, Time: 51.15s\n",
      "Epoch 18/50, Train Loss: 6269.7068, Val Loss: 237.0679, Val Error: 10.80%, Test Error: 10.03%, Time: 49.66s\n",
      "Epoch 19/50, Train Loss: 6182.9373, Val Loss: 233.7231, Val Error: 10.74%, Test Error: 10.04%, Time: 50.58s\n",
      "Epoch 20/50, Train Loss: 6097.1781, Val Loss: 230.3963, Val Error: 10.88%, Test Error: 10.60%, Time: 49.82s\n",
      "Epoch 21/50, Train Loss: 6012.7222, Val Loss: 227.1253, Val Error: 10.99%, Test Error: 10.51%, Time: 49.74s\n",
      "Epoch 22/50, Train Loss: 5929.2012, Val Loss: 223.8968, Val Error: 11.43%, Test Error: 10.28%, Time: 49.06s\n",
      "Epoch 23/50, Train Loss: 5847.7279, Val Loss: 220.7512, Val Error: 11.17%, Test Error: 10.55%, Time: 50.48s\n",
      "Epoch 24/50, Train Loss: 5768.2698, Val Loss: 217.6653, Val Error: 11.52%, Test Error: 10.29%, Time: 50.72s\n",
      "Epoch 25/50, Train Loss: 5691.1208, Val Loss: 214.6978, Val Error: 11.33%, Test Error: 11.00%, Time: 50.20s\n",
      "Epoch 26/50, Train Loss: 5617.2707, Val Loss: 211.8590, Val Error: 12.09%, Test Error: 11.44%, Time: 49.03s\n",
      "Epoch 27/50, Train Loss: 5546.7487, Val Loss: 209.1744, Val Error: 11.38%, Test Error: 11.40%, Time: 50.52s\n",
      "Epoch 28/50, Train Loss: 5479.8292, Val Loss: 206.6112, Val Error: 11.97%, Test Error: 11.32%, Time: 51.08s\n",
      "Epoch 29/50, Train Loss: 5417.0738, Val Loss: 204.2619, Val Error: 12.63%, Test Error: 12.04%, Time: 54.34s\n",
      "Epoch 30/50, Train Loss: 5357.7715, Val Loss: 202.0216, Val Error: 13.13%, Test Error: 11.80%, Time: 52.17s\n",
      "Epoch 31/50, Train Loss: 5302.8466, Val Loss: 199.9363, Val Error: 13.00%, Test Error: 11.93%, Time: 48.20s\n",
      "Epoch 32/50, Train Loss: 5251.4374, Val Loss: 197.9986, Val Error: 13.50%, Test Error: 11.93%, Time: 50.48s\n",
      "Epoch 33/50, Train Loss: 5204.2041, Val Loss: 196.2133, Val Error: 12.91%, Test Error: 12.47%, Time: 49.35s\n",
      "Epoch 34/50, Train Loss: 5160.2859, Val Loss: 194.5537, Val Error: 13.59%, Test Error: 12.89%, Time: 51.73s\n",
      "Epoch 35/50, Train Loss: 5119.9839, Val Loss: 193.0699, Val Error: 13.62%, Test Error: 12.16%, Time: 49.45s\n",
      "Epoch 36/50, Train Loss: 5083.2118, Val Loss: 191.6795, Val Error: 13.94%, Test Error: 12.17%, Time: 48.59s\n",
      "Epoch 37/50, Train Loss: 5048.9912, Val Loss: 190.4016, Val Error: 13.92%, Test Error: 13.44%, Time: 50.47s\n",
      "Epoch 38/50, Train Loss: 5017.7403, Val Loss: 189.2115, Val Error: 14.14%, Test Error: 12.82%, Time: 50.49s\n",
      "Epoch 39/50, Train Loss: 4989.0790, Val Loss: 188.1710, Val Error: 13.65%, Test Error: 13.17%, Time: 48.74s\n",
      "Epoch 40/50, Train Loss: 4962.7479, Val Loss: 187.1607, Val Error: 14.45%, Test Error: 13.29%, Time: 49.15s\n",
      "Epoch 41/50, Train Loss: 4938.9707, Val Loss: 186.2551, Val Error: 13.73%, Test Error: 13.38%, Time: 49.89s\n",
      "Epoch 42/50, Train Loss: 4916.9563, Val Loss: 185.4583, Val Error: 13.62%, Test Error: 12.86%, Time: 50.01s\n",
      "Epoch 43/50, Train Loss: 4896.8966, Val Loss: 184.7045, Val Error: 13.65%, Test Error: 13.56%, Time: 49.24s\n",
      "Epoch 44/50, Train Loss: 4878.6403, Val Loss: 184.0331, Val Error: 13.72%, Test Error: 12.99%, Time: 50.21s\n",
      "Epoch 45/50, Train Loss: 4861.8957, Val Loss: 183.3967, Val Error: 13.80%, Test Error: 13.63%, Time: 50.42s\n",
      "Epoch 46/50, Train Loss: 4846.6319, Val Loss: 182.8300, Val Error: 13.59%, Test Error: 12.94%, Time: 49.25s\n",
      "Epoch 47/50, Train Loss: 4832.6632, Val Loss: 182.3069, Val Error: 13.68%, Test Error: 13.67%, Time: 50.05s\n",
      "Epoch 48/50, Train Loss: 4819.9415, Val Loss: 181.8370, Val Error: 14.38%, Test Error: 12.97%, Time: 49.20s\n",
      "Epoch 49/50, Train Loss: 4808.1695, Val Loss: 181.4011, Val Error: 13.66%, Test Error: 12.50%, Time: 50.56s\n",
      "Epoch 50/50, Train Loss: 4797.7925, Val Loss: 181.0160, Val Error: 14.14%, Test Error: 13.16%, Time: 49.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading config.yaml\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch ▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_error █▅▅▅▅▅▃▂▂▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▂▃▃▃▂▃▃▂▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss █▆▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_error █▅▄▅▄▄▂▂▁▂▁▁▁▁▁▁▁▁▂▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss █▇▆▅▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      epoch 49\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: test_error 0.1316\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 4797.79245\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  val_error 0.1414\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val_loss 181.01595\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mmnist\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper/runs/so7m9drh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/matteo-ghia-2001/asi-paper\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250529_131214-so7m9drh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "key = user_secrets.get_secret('wand-api-key-asi')\n",
    "\n",
    "wandb.login(key=key)\n",
    "\n",
    "# model = train_mnist(train_loader, val_loader, test_loader, epochs=50, lr=1e-3, num_samples=3, pi=0.5, minus_log_sigma1=0, minus_log_sigma2=6, use_wandb=True)\n",
    "model = train_final(full_train_loader, test_loader)\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53322c7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.729539Z",
     "iopub.status.busy": "2025-05-29T13:54:02.729290Z",
     "iopub.status.idle": "2025-05-29T13:54:02.732643Z",
     "shell.execute_reply": "2025-05-29T13:54:02.732090Z"
    },
    "papermill": {
     "duration": 0.011801,
     "end_time": "2025-05-29T13:54:02.733698",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.721897",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = MNISTModel(prior_sigma_1=np.exp(-1), prior_sigma_2=np.exp(-7), prior_pi=0.75)\n",
    "# model.to(device)\n",
    "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ada23dd",
   "metadata": {
    "papermill": {
     "duration": 0.00621,
     "end_time": "2025-05-29T13:54:02.746236",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.740026",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Regression curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6ea77f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.759833Z",
     "iopub.status.busy": "2025-05-29T13:54:02.759628Z",
     "iopub.status.idle": "2025-05-29T13:54:02.765558Z",
     "shell.execute_reply": "2025-05-29T13:54:02.765039Z"
    },
    "papermill": {
     "duration": 0.013894,
     "end_time": "2025-05-29T13:54:02.766489",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.752595",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_samples(num_samples):\n",
    "    eps = np.random.normal(0, 0.02, num_samples)\n",
    "    x = np.linspace(0, 0.5, num_samples)\n",
    "    y = x + 0.3 * np.sin(2 * np.pi * (x + eps)) + 0.3 * np.sin(4 * np.pi * (x + eps))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def save_samples(x, y, filename):\n",
    "    df = pd.DataFrame({'x': x, 'y': y})\n",
    "    df.to_csv(filename, index=False)\n",
    "\n",
    "\n",
    "def load_samples(filename):\n",
    "    df = pd.read_csv(filename)\n",
    "    x = df['x'].values\n",
    "    y = df['y'].values\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def plot_samples(x, y):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, y, 'kx', label='Generated Samples')\n",
    "    plt.title('Generated Samples')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# x, y = generate_samples(1000)\n",
    "# save_samples(x, y, 'regression_samples.csv')\n",
    "\n",
    "\n",
    "# x, y = load_samples('regression_samples.csv')\n",
    "# plot_samples(x, y)\n",
    "\n",
    "# X_tensor = torch.tensor(x, dtype=torch.float32).view(-1, 1).to(device)\n",
    "# y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# train_dataset = torch.utils.data.TensorDataset(X_tensor[:800], y_tensor[:800])\n",
    "# val_dataset = torch.utils.data.TensorDataset(X_tensor[800:], y_tensor[800:])\n",
    "\n",
    "\n",
    "# kwargs = {\n",
    "#     'batch_size': batch_size,\n",
    "#     'generator': generator,\n",
    "# }\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(\n",
    "#     train_dataset,\n",
    "#     shuffle=True,\n",
    "#     **kwargs\n",
    "# )\n",
    "# val_loader = torch.utils.data.DataLoader(\n",
    "#     val_dataset,\n",
    "#     shuffle=False,\n",
    "#     **kwargs\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242d0755",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.780090Z",
     "iopub.status.busy": "2025-05-29T13:54:02.779707Z",
     "iopub.status.idle": "2025-05-29T13:54:02.784131Z",
     "shell.execute_reply": "2025-05-29T13:54:02.783640Z"
    },
    "papermill": {
     "duration": 0.012306,
     "end_time": "2025-05-29T13:54:02.785075",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.772769",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, in_features=1, out_features=1, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                200,\n",
    "                200,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                200,\n",
    "                out_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b658477",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.798686Z",
     "iopub.status.busy": "2025-05-29T13:54:02.798485Z",
     "iopub.status.idle": "2025-05-29T13:54:02.804281Z",
     "shell.execute_reply": "2025-05-29T13:54:02.803788Z"
    },
    "papermill": {
     "duration": 0.013855,
     "end_time": "2025-05-29T13:54:02.805381",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.791526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_regression(regressor, X, y, samples=100, std_multiplier=2):\n",
    "    preds = [regressor(X) for _ in range(samples)]\n",
    "    preds = torch.stack(preds)\n",
    "    means = preds.mean(axis=0)\n",
    "    stds = preds.std(axis=0)\n",
    "    ci_upper = means + (std_multiplier * stds)\n",
    "    ci_lower = means - (std_multiplier * stds)\n",
    "    ci_acc = (ci_lower <= y) * (ci_upper >= y)\n",
    "    ci_acc = ci_acc.float().mean()\n",
    "    return ci_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()\n",
    "\n",
    "\n",
    "def train_regression(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        ci_acc, ci_upper, ci_lower = evaluate_regression(model, val_loader.dataset.tensors[0], val_loader.dataset.tensors[1])\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"ci_acc\": ci_acc,\n",
    "                \"ci_upper\": ci_upper,\n",
    "                \"ci_lower\": ci_lower,\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, CI acc: {ci_acc}, CI upper acc: {ci_upper}, CI lower acc: {ci_lower} Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d74da05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.819296Z",
     "iopub.status.busy": "2025-05-29T13:54:02.819101Z",
     "iopub.status.idle": "2025-05-29T13:54:02.823642Z",
     "shell.execute_reply": "2025-05-29T13:54:02.822958Z"
    },
    "papermill": {
     "duration": 0.01255,
     "end_time": "2025-05-29T13:54:02.824680",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.812130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_regression_model(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = RegressionModel(1, 1, prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     run = wandb.init(project=\"asi-paper\", name=\"regression\")\n",
    "\n",
    "    train_regression(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     run.finish()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3208d450",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.838156Z",
     "iopub.status.busy": "2025-05-29T13:54:02.837962Z",
     "iopub.status.idle": "2025-05-29T13:54:02.841840Z",
     "shell.execute_reply": "2025-05-29T13:54:02.841303Z"
    },
    "papermill": {
     "duration": 0.011718,
     "end_time": "2025-05-29T13:54:02.842821",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.831103",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret('wand-api-key-asi')\n",
    "\n",
    "# wandb.login(key=key)\n",
    "\n",
    "\n",
    "# def train_wrapper():\n",
    "#     with wandb.init(project=\"asi-paper\") as run:\n",
    "#         model = train_regression_model(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             epochs=15,\n",
    "#             lr=run.config.lr,\n",
    "#             num_samples=run.config.sample_nbr,\n",
    "#             pi=run.config.pi,\n",
    "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
    "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
    "#             use_wandb=True\n",
    "#         )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# sweep_configuration = {\n",
    "#     \"method\": \"bayes\",\n",
    "#     \"metric\": {\"goal\": \"maximize\", \"name\": \"ci_acc\"},\n",
    "#     'name': \"sweep-regression\",\n",
    "#     \"parameters\": {\n",
    "#         \"lr\": {'min': 1e-5, 'max': 1e-2},\n",
    "#         \"sample_nbr\": {'min': 1, 'max': 10},\n",
    "#         \"pi\": {'min': 0.25, 'max': 0.75},\n",
    "#         \"min_log_sigma1\": {'min': 0, 'max': 2},\n",
    "#         \"min_log_sigma2\": {'min': 6, 'max': 8},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "# wandb.agent(sweep_id, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "900b324f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.856142Z",
     "iopub.status.busy": "2025-05-29T13:54:02.855939Z",
     "iopub.status.idle": "2025-05-29T13:54:02.858913Z",
     "shell.execute_reply": "2025-05-29T13:54:02.858440Z"
    },
    "papermill": {
     "duration": 0.010763,
     "end_time": "2025-05-29T13:54:02.859896",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.849133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = train_regression_model(train_loader, val_loader, epochs=10, lr=1e-3, num_samples=1, pi=0.5, minus_log_sigma1=0.5, minus_log_sigma2=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149a5a3c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T13:54:02.875051Z",
     "iopub.status.busy": "2025-05-29T13:54:02.874197Z",
     "iopub.status.idle": "2025-05-29T13:54:02.878104Z",
     "shell.execute_reply": "2025-05-29T13:54:02.877434Z"
    },
    "papermill": {
     "duration": 0.012677,
     "end_time": "2025-05-29T13:54:02.879266",
     "exception": false,
     "start_time": "2025-05-29T13:54:02.866589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "# predicted = model(X_tensor).cpu().detach().numpy()\n",
    "\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.plot(x, y, 'kx', label='Generated Samples')\n",
    "# plt.plot(x, predicted, 'r-', label='Predicted Mean')\n",
    "# # plt.fill_between(x, predicted - 2 * np.std(predicted), predicted + 2 * np.std(predicted), color='r', alpha=0.2, label='Uncertainty')\n",
    "# plt.title('Regression with Uncertainty')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2538.362299,
   "end_time": "2025-05-29T13:54:05.605515",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-05-29T13:11:47.243216",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

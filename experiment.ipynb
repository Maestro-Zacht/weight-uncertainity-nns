{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:08.496914Z",
     "iopub.status.busy": "2025-05-13T13:54:08.496227Z",
     "iopub.status.idle": "2025-05-13T13:54:18.188437Z",
     "shell.execute_reply": "2025-05-13T13:54:18.187899Z",
     "shell.execute_reply.started": "2025-05-13T13:54:08.496880Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import v2 as transforms\n",
    "import numpy as np\n",
    "import wandb\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = torch.Generator().manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:18.190106Z",
     "iopub.status.busy": "2025-05-13T13:54:18.189724Z",
     "iopub.status.idle": "2025-05-13T13:54:18.202374Z",
     "shell.execute_reply": "2025-05-13T13:54:18.201575Z",
     "shell.execute_reply.started": "2025-05-13T13:54:18.190088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class GaussianVariational(nn.Module):\n",
    "\n",
    "    def __init__(self, mu: torch.Tensor, rho: torch.Tensor) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = nn.Parameter(mu)\n",
    "        self.rho = nn.Parameter(rho)\n",
    "\n",
    "        self.w = None\n",
    "        self.sigma = None\n",
    "\n",
    "        self.normal = torch.distributions.Normal(0, 1)\n",
    "\n",
    "    def sample(self) -> torch.Tensor:\n",
    "        device = self.mu.device\n",
    "        epsilon = self.normal.sample(self.mu.size()).to(device)\n",
    "        self.sigma = torch.log(1 + torch.exp(self.rho)).to(device)\n",
    "        self.w = self.mu + self.sigma * epsilon\n",
    "\n",
    "        return self.w\n",
    "\n",
    "    def log_posterior(self) -> torch.Tensor:\n",
    "        assert self.w is not None\n",
    "\n",
    "        log_const = np.log(np.sqrt(2 * np.pi))\n",
    "        log_exp = ((self.w - self.mu) ** 2) / (2 * self.sigma ** 2)\n",
    "        log_posterior = -log_const - torch.log(self.sigma) - log_exp\n",
    "\n",
    "        return log_posterior.sum()\n",
    "\n",
    "\n",
    "class ScaleMixture(nn.Module):\n",
    "\n",
    "    def __init__(self, pi: float, sigma1: float, sigma2: float) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.pi = pi\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "\n",
    "        self.normal1 = torch.distributions.Normal(0, sigma1)\n",
    "        self.normal2 = torch.distributions.Normal(0, sigma2)\n",
    "\n",
    "    def log_prior(self, w: torch.Tensor) -> torch.Tensor:\n",
    "        likelihood_n1 = torch.exp(self.normal1.log_prob(w))\n",
    "        likelihood_n2 = torch.exp(self.normal2.log_prob(w))\n",
    "\n",
    "        p_scalemixture = self.pi * likelihood_n1 + (1 - self.pi) * likelihood_n2\n",
    "        log_prob = torch.log(p_scalemixture).sum()\n",
    "\n",
    "        return log_prob\n",
    "\n",
    "\n",
    "class BayesianModule(nn.Module):\n",
    "    pass\n",
    "\n",
    "\n",
    "class BayesLinear(BayesianModule):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 prior_pi: Optional[float] = 0.5,\n",
    "                 prior_sigma1: Optional[float] = 1.0,\n",
    "                 prior_sigma2: Optional[float] = 0.0025) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        w_mu = torch.empty(out_features, in_features).uniform_(-0.2, 0.2, generator=generator)\n",
    "        w_rho = torch.empty(out_features, in_features).uniform_(-5.0, -4.0, generator=generator)\n",
    "\n",
    "        bias_mu = torch.empty(out_features).uniform_(-0.2, 0.2, generator=generator)\n",
    "        bias_rho = torch.empty(out_features).uniform_(-5.0, -4.0, generator=generator)\n",
    "\n",
    "        self.w_posterior = GaussianVariational(w_mu, w_rho)\n",
    "        self.bias_posterior = GaussianVariational(bias_mu, bias_rho)\n",
    "\n",
    "        self.w_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "        self.bias_prior = ScaleMixture(prior_pi, prior_sigma1, prior_sigma2)\n",
    "\n",
    "        self.kl_divergence = 0.0\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        w = self.w_posterior.sample()\n",
    "        b = self.bias_posterior.sample()\n",
    "\n",
    "        w_log_prior = self.w_prior.log_prior(w)\n",
    "        b_log_prior = self.bias_prior.log_prior(b)\n",
    "\n",
    "        w_log_posterior = self.w_posterior.log_posterior()\n",
    "        b_log_posterior = self.bias_posterior.log_posterior()\n",
    "\n",
    "        total_log_prior = w_log_prior + b_log_prior\n",
    "        total_log_posterior = w_log_posterior + b_log_posterior\n",
    "        self.kl_divergence = total_log_posterior - total_log_prior\n",
    "\n",
    "        return F.linear(x, w, b)\n",
    "\n",
    "\n",
    "def minibatch_weight(batch_idx: int, num_batches: int) -> float:\n",
    "    return 2 ** (num_batches - batch_idx) / (2 ** num_batches - batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:18.236903Z",
     "iopub.status.busy": "2025-05-13T13:54:18.236718Z",
     "iopub.status.idle": "2025-05-13T13:54:18.303530Z",
     "shell.execute_reply": "2025-05-13T13:54:18.302959Z",
     "shell.execute_reply.started": "2025-05-13T13:54:18.236889Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:18.203504Z",
     "iopub.status.busy": "2025-05-13T13:54:18.203202Z",
     "iopub.status.idle": "2025-05-13T13:54:18.236030Z",
     "shell.execute_reply": "2025-05-13T13:54:18.235554Z",
     "shell.execute_reply.started": "2025-05-13T13:54:18.203479Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MNISTModel(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features=28 * 28, out_features=10, prior_sigma_1=0.1, prior_sigma_2=0.4, prior_pi=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                in_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                in_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            BayesLinear(\n",
    "                in_features,\n",
    "                out_features,\n",
    "                prior_pi,\n",
    "                prior_sigma_1,\n",
    "                prior_sigma_2,\n",
    "            ),\n",
    "            nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        # print(x)\n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def kl_divergence(self):\n",
    "        kl = 0\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, BayesianModule):\n",
    "                kl += module.kl_divergence\n",
    "\n",
    "        return kl\n",
    "\n",
    "    def sample_elbo(self, inputs, labels, criterion, num_samples, complexity_cost_weight=1):\n",
    "        loss = 0\n",
    "        for _ in range(num_samples):\n",
    "            outputs = self(inputs)\n",
    "            contr1 = criterion(outputs, labels)\n",
    "            contr2 = self.kl_divergence * complexity_cost_weight\n",
    "            # print(f\"contr1: {contr1}, contr2: {contr2}\")\n",
    "            loss += contr1 + contr2\n",
    "        return loss / num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:18.305444Z",
     "iopub.status.busy": "2025-05-13T13:54:18.305217Z",
     "iopub.status.idle": "2025-05-13T13:54:18.322476Z",
     "shell.execute_reply": "2025-05-13T13:54:18.321946Z",
     "shell.execute_reply.started": "2025-05-13T13:54:18.305420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, optimizer, criterion, num_samples=1):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        kl_weight = minibatch_weight(batch_idx, len(train_loader))\n",
    "\n",
    "        loss = model.sample_elbo(data, target, criterion, num_samples, kl_weight)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target) + model.kl_divergence * minibatch_weight(batch_idx, len(test_loader))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(output, 1)\n",
    "            correct += (preds == target).sum().item()\n",
    "\n",
    "            # print(f\"Predictions: {preds}, Targets: {target}\")\n",
    "\n",
    "    total = len(test_loader.dataset)\n",
    "    error = (total - correct) / total\n",
    "\n",
    "    print(f\"Correct: {correct}/{total} ({correct / total:.2%})\")\n",
    "    return total_loss / total, error\n",
    "\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer, criterion, num_epochs, num_samples, use_wandb=False):\n",
    "    for epoch in range(num_epochs):\n",
    "        now = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, criterion, num_samples)\n",
    "        val_loss, val_error = evaluate(model, val_loader, criterion)\n",
    "\n",
    "        elapsed = time.time() - now\n",
    "\n",
    "        if use_wandb:\n",
    "            wandb.log({\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_error\": val_error\n",
    "            })\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Error: {val_error:.4f}, Time: {elapsed:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:18.323373Z",
     "iopub.status.busy": "2025-05-13T13:54:18.323142Z",
     "iopub.status.idle": "2025-05-13T13:54:18.337588Z",
     "shell.execute_reply": "2025-05-13T13:54:18.336887Z",
     "shell.execute_reply.started": "2025-05-13T13:54:18.323353Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_mnist(train_loader, val_loader, epochs, lr, num_samples, pi, minus_log_sigma1, minus_log_sigma2, use_wandb=False):\n",
    "    sigma1 = np.exp(-minus_log_sigma1)\n",
    "    sigma2 = np.exp(-minus_log_sigma2)\n",
    "\n",
    "    model = MNISTModel(prior_sigma_1=sigma1, prior_sigma_2=sigma2, prior_pi=pi)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "    # if use_wandb:\n",
    "    #     wandb.init(project=\"asi-paper\", name=\"mnist\")\n",
    "\n",
    "    train(model, train_loader, val_loader, optimizer, criterion, epochs, num_samples, use_wandb=use_wandb)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:55:10.263892Z",
     "iopub.status.busy": "2025-05-13T13:55:10.263590Z",
     "iopub.status.idle": "2025-05-13T13:55:10.355141Z",
     "shell.execute_reply": "2025-05-13T13:55:10.354536Z",
     "shell.execute_reply.started": "2025-05-13T13:55:10.263865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    transforms.Lambda(lambda x: x.view(28 * 28)),\n",
    "])\n",
    "\n",
    "\n",
    "mnist_dataset = datasets.MNIST(\n",
    "    root=\"./mnist\",\n",
    "    download=True,\n",
    "    transform=transform,\n",
    "    train=True\n",
    ")\n",
    "# transformed_data = transform(mnist_dataset.data).to(device)\n",
    "# y = mnist_dataset.targets.to(device)\n",
    "# mnist_dataset = torch.utils.data.TensorDataset(transformed_data, y)\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(mnist_dataset, [50_000, 10_000], generator=generator)\n",
    "\n",
    "kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'num_workers': 4,\n",
    "    'generator': generator,\n",
    "}\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    **kwargs\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset,\n",
    "    shuffle=False,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search with wandb\n",
    "Uncomment the code below to run a grid search and log the results to wandb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:54:22.194662Z",
     "iopub.status.busy": "2025-05-13T13:54:22.194421Z",
     "iopub.status.idle": "2025-05-13T13:54:22.198249Z",
     "shell.execute_reply": "2025-05-13T13:54:22.197481Z",
     "shell.execute_reply.started": "2025-05-13T13:54:22.194636Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from kaggle_secrets import UserSecretsClient\n",
    "# user_secrets = UserSecretsClient()\n",
    "# key = user_secrets.get_secret('wand-api-key-asi')\n",
    "\n",
    "# wandb.login(key=key)\n",
    "\n",
    "\n",
    "# def train_wrapper():\n",
    "#     with wandb.init(project=\"asi-paper\") as run:\n",
    "#         model = train_mnist(\n",
    "#             train_loader,\n",
    "#             val_loader,\n",
    "#             epochs=10,\n",
    "#             lr=run.config.lr,\n",
    "#             num_samples=run.config.sample_nbr,\n",
    "#             pi=run.config.pi,\n",
    "#             minus_log_sigma1=run.config.min_log_sigma1,\n",
    "#             minus_log_sigma2=run.config.min_log_sigma2,\n",
    "#             use_wandb=True\n",
    "#         )\n",
    "\n",
    "#     return model\n",
    "\n",
    "\n",
    "# sweep_configuration = {\n",
    "#     \"method\": \"grid\",\n",
    "#     \"metric\": {\"goal\": \"minimize\", \"name\": \"val_error\"},\n",
    "#     'name': \"sweep-mnist\",\n",
    "#     \"parameters\": {\n",
    "#         \"lr\": {'values': [1e-3, 1e-4, 1e-5]},\n",
    "#         \"sample_nbr\": {'values': [1, 2, 5, 10]},\n",
    "#         \"pi\": {'values': [0.25, 0.5, 0.75]},\n",
    "#         \"min_log_sigma1\": {'values': [0, 1, 2]},\n",
    "#         \"min_log_sigma2\": {'values': [6, 7, 8]},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "# sweep_id = wandb.sweep(sweep=sweep_configuration, project=\"asi-paper\")\n",
    "# wandb.agent(sweep_id, function=train_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual training\n",
    "Uncomment the code below to train the model with specified hyperparameters and save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-13T13:55:14.261842Z",
     "iopub.status.busy": "2025-05-13T13:55:14.261068Z",
     "iopub.status.idle": "2025-05-13T13:55:48.075758Z",
     "shell.execute_reply": "2025-05-13T13:55:48.074644Z",
     "shell.execute_reply.started": "2025-05-13T13:55:14.261809Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = train_mnist(train_loader, val_loader, epochs=20, lr=1e-3, num_samples=1, pi=0.75, minus_log_sigma1=1, minus_log_sigma2=7)\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-13T13:54:58.599751Z",
     "iopub.status.idle": "2025-05-13T13:54:58.599980Z",
     "shell.execute_reply": "2025-05-13T13:54:58.599871Z",
     "shell.execute_reply.started": "2025-05-13T13:54:58.599861Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"mnist_model.pt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression curves"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
